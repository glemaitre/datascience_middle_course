{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cb79d40-9375-44d9-94bc-b5a8f518ed12",
   "metadata": {},
   "source": [
    "# Boosting-based estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e80a5d-0d74-45e9-88b0-c0bff33d9e18",
   "metadata": {},
   "source": [
    "In this notebook, we will present a second family of ensemble method known as boosting. We will first give an intuitive example of how boosting works. It will follow by an introduction to gradient boosting decision tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a117d77a-1553-444a-b297-338664affb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary fix to avoid spurious warning raised in scikit-learn 1.0.0\n",
    "# it will be solved in scikit-learn 1.0.1\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X has feature names.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba849687-d4f5-4a49-92e7-46b7f64c25b8",
   "metadata": {},
   "source": [
    "## Introduction to boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f6088-c0cf-48f8-b0e6-cd1cf35c8459",
   "metadata": {},
   "source": [
    "We will first give an intuive explanation on the principle of boosting. In the previous notebook, we saw that bagging was creating several datasets with a little variation using bootstrapping. Then an estimator was trained on each of these different datasets and the different results were aggregated. In boosting, the paradigm is different: the estimators will be trained on the same dataset. Thus, to combine them, we will train an estimator to correct the error of all previous estimators. So, we have a sequence of estimators instead of independent estimators.\n",
    "\n",
    "Let's give an example on a classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4012bc-74e6-4bad-91a9-c6c20b1f475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../datasets/penguins_classification.csv\")\n",
    "data[\"Species\"] = data[\"Species\"].astype(\"category\")\n",
    "X, y = data[[\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]], data[\"Species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7825f933-04f0-4b29-bc01-e22c0582a1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bcb764-6f5f-4676-b75c-00781ecb2a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "_ = data.plot.scatter(\n",
    "    x=\"Culmen Length (mm)\",\n",
    "    y=\"Culmen Depth (mm)\",\n",
    "    c=\"Species\",\n",
    "    s=80,\n",
    "    cmap=plt.cm.viridis,\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6048e7-e2cd-4282-aaec-e4384a95a858",
   "metadata": {},
   "source": [
    "In this dataset, we have three species of penguin and we want to distinguish them based on the culmen depth and length.\n",
    "Let's start to train a shallow decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff6f8e-e2b5-4a6f-9712-f2c67abd1f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58efa8b-2d5c-4ac0-9c56-56b5c7a5737e",
   "metadata": {},
   "source": [
    "We will check qualitatively the statistical performance of our model by looking at the decision boundary and point-out the misclassified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b16a4-f7fe-4208-b4cc-934de8c8de4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "target_predicted = tree.predict(X)\n",
    "mask_misclassified = y != target_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d06d4-eba0-42ee-b4ae-1ca982f6281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.plotting import DecisionBoundaryDisplay\n",
    "\n",
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# plot the decision boundaries\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    tree, X, response_method=\"predict\", cmap=plt.cm.viridis,\n",
    "    alpha=0.4, ax=ax,\n",
    ")\n",
    "\n",
    "# plot the original dataset\n",
    "data.plot.scatter(\n",
    "    x=\"Culmen Length (mm)\",\n",
    "    y=\"Culmen Depth (mm)\",\n",
    "    c=\"Species\",\n",
    "    s=80,\n",
    "    cmap=plt.cm.viridis,\n",
    "    alpha=0.5,\n",
    "    edgecolor=\"black\",\n",
    "    ax=ax,\n",
    ")\n",
    "# plot the misclassified samples\n",
    "data[mask_misclassified].plot.scatter(\n",
    "    x=\"Culmen Length (mm)\",\n",
    "    y=\"Culmen Depth (mm)\",\n",
    "    c=\"black\",\n",
    "    s=200,\n",
    "    marker=\"+\",\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = plt.title(\"Decision tree predictions \\nwith misclassified samples \"\n",
    "              \"highlighted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da83e5-fda7-481e-af31-f6944b40282c",
   "metadata": {},
   "source": [
    "We observe that our decision tree is making a couple of error for some Gentoo and Adelie samples. What we would be interested in now is to train a new decision tree but that should only focus on the misclassified samples this time. Scikit-learn exposes a `sample_weight` parameter in the `fit` method that allows to give more weight to some specific sample. We will use this parameter to only focus our new decision tree on the misclassified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f6382-e492-44c9-ad24-faf18486e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = mask_misclassified.astype(np.float64)\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=2, random_state=0)\n",
    "tree.fit(X, y, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133310e1-72c7-4cbc-acc7-4392055afbb2",
   "metadata": {},
   "source": [
    "Let's check the decision boundary of this newly trained decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd2ae0-b1d6-49b5-bba6-ffbb0d5233bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# plot the decision boundaries\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    tree, X, response_method=\"predict\", cmap=plt.cm.viridis,\n",
    "    alpha=0.4,\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "# plot the original dataset\n",
    "data.plot.scatter(\n",
    "    x=\"Culmen Length (mm)\",\n",
    "    y=\"Culmen Depth (mm)\",\n",
    "    c=\"Species\",\n",
    "    s=80,\n",
    "    cmap=plt.cm.viridis,\n",
    "    alpha=0.5,\n",
    "    edgecolor=\"black\",\n",
    "    ax=ax,\n",
    ")\n",
    "# plot the misclassified samples\n",
    "data[mask_misclassified].plot.scatter(\n",
    "    x=\"Culmen Length (mm)\",\n",
    "    y=\"Culmen Depth (mm)\",\n",
    "    c=\"black\",\n",
    "    s=200,\n",
    "    marker=\"+\",\n",
    "    ax=ax,\n",
    ")\n",
    "\n",
    "_ = plt.title(\"Decision tree predictions \\nwith misclassified samples \"\n",
    "              \"highlighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e17339-6878-412c-9b54-9efb4ee05c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_predicted = tree.predict(X)\n",
    "mask_new_misclassifier = y != target_predicted\n",
    "remaining_misclassified_samples_idx = (\n",
    "    mask_misclassified & mask_new_misclassifier\n",
    ")\n",
    "\n",
    "print(f\"Number of samples previously misclassified and \"\n",
    "      f\"still misclassified: {remaining_misclassified_samples_idx.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2338a6-70d1-4345-858e-d36f07a4a183",
   "metadata": {},
   "source": [
    "We can observe that the previously misclassified samples are well classified now. However, it comes at the cost of misclassifying some other samples. We could continue by training a serie of decision tree classifiers. However, at some point, we need to find a way to combine them. One way could be that we could trust more or less a classifier depending on the ratio of good classification on the full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec6422-036c-415f-bdf2-17900f2eadce",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_weight = [\n",
    "    (y.size - mask_misclassified.sum()) / y.size,\n",
    "    (y.size - mask_new_misclassifier.sum()) / y.size,\n",
    "]\n",
    "ensemble_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a6106b-9aa0-46fd-aafd-f5f3e6bb38a6",
   "metadata": {},
   "source": [
    "In our example, the first classification has a good accuracy and we will trust it more than the second classifier. Thus, we could make a linear combination of the different decision tree classifiers.\n",
    "\n",
    "The algorithm that we just did is a simplification of an algorithm known as `AdaBoostClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256a34c-0e6e-486b-99d4-e21b2659e5ce",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE:</b>:\n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Train a <tt>sklearn.ensemble.AdaBoostClassifier</tt> with 3 estimators and where the base estimator is a <tt>DecisionTreeClassifier</tt> with a <tt>max_depth=3</tt>.</li>\n",
    "        <li>Once this classifier trained, access the fitted attribute <tt>estimators_</tt> that contains the different decision tree classifiers and plot their decision boundary.</li>\n",
    "        <li>What are the weights associated with each decision tree classifiers.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a435de4-1e94-40cb-9d1f-ef13d56aa852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_20.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331b041-326d-48dc-ad6d-85a13d0661d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_21.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77433c88-f0cd-465c-9d7b-37510ebaf979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_22.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcd6356-1fb7-46be-b37c-990ef34a5c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_23.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1544a272-4ccf-4b38-8d06-b2a8a7f5baee",
   "metadata": {},
   "source": [
    "## Gradient Boosting Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09576f-c60e-4af0-82bc-ae5b8f40cb9e",
   "metadata": {},
   "source": [
    "AdaBoost predictors are less use nowadays in practice. Instead, gradient boosting decision trees are used and have been demonstrated to be better models.\n",
    "\n",
    "In gradient boosting, each estimator will be a decision tree regressor even in classification. Using a regression tree allows to get a continuous residuals. Each estimator to be added in the sequence of estimator will be trained on the residuals of the previous estimators. In addition, there are a couple of parameters allowing to correct more or less fast the residuals from previous estimators.\n",
    "\n",
    "Let's illustrate such model on a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1efe4e-7920-4fc0-8518-100567ca6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv(\"../datasets/adult-census-numeric-all.csv\")\n",
    "X, y = data.drop(columns=\"class\"), data[\"class\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dd705e-81bd-48ae-936c-3333295ef9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "classifier = GradientBoostingClassifier(n_estimators=5)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a79b9bc-8f70-4723-8ee5-c44bebe40eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80026fc-2007-4d89-b65a-9e2260232890",
   "metadata": {},
   "source": [
    "We can inspect the different underlying estimators to show that we used indeed decisiont tree regressor even in a classification setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cfb305-40ef-41c5-8149-d0710776c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41dc3ff-8eca-40ee-a9dd-49862d56ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "_, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "_ = plot_tree(\n",
    "    classifier.estimators_[0][0],\n",
    "    feature_names=X_train.columns,\n",
    "    ax=ax,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889b523-ceed-4f8e-80a5-bf6192bfda28",
   "metadata": {},
   "source": [
    "### Histogram gradient boosting decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6693fa0-2236-4ee9-97b4-905a68388e25",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>ACCELERATE GRADIENT BOOSTING</b>:\n",
    "    <ul>\n",
    "        <li>Which solution would you use to accelerate the training speed of gradient boosting algorithm.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239f859a-3e44-4d26-baf9-2f7e2394b15e",
   "metadata": {},
   "source": [
    "## Short introduction of `KBinsDiscretizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5184d5d-795a-45ba-8f76-f71cc9a7a18c",
   "metadata": {},
   "source": [
    "We will show a trick to accelerate gradient boosting and more generally decision tree. When presenting decision trees, we mentioned that a split is chosen among all possible available splits that are defined by the unique values available in a given feature. One can reduce the amount of splits by binning the values of a feature beforehand and only consider the bin edges as potential edge. Since gradient boosting is ensembling several model, the lack of available splits will be attenuated by the size of the ensemble.\n",
    "\n",
    "Here, we show that you can bin a dataset in scikit-learn using the `KBinsDiscritizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db5739-3b9f-4a30-ab7a-e13a531a02f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "discretizer = KBinsDiscretizer(\n",
    "    n_bins=10, encode=\"ordinal\", strategy=\"uniform\"\n",
    ")\n",
    "X_trans = discretizer.fit_transform(X)\n",
    "X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7a962-f422-4cfa-a800-158c01f31168",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(np.unique(col)) for col in X_trans.T]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1300ab89-b423-4fdb-9bbb-953afb221541",
   "metadata": {},
   "source": [
    "Here, we decided to use 10 bins for each features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a6c910-6627-4a00-8c6f-ed1ffef82e6e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "    <ul>\n",
    "        <li>Create a pipeline composed of a <tt>KBinsDiscretizer</tt> followed by a <tt>GradientBoostingClassfier</tt>.</li>\n",
    "        <li>Compare the training time with the vanilla <tt>GradientBoostingClassifier</tt>.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c55efe6-cb6a-42d5-b143-16b5ecc8b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_24.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a626bca-a3b5-4ae0-a07e-e221b7a96e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_25.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f437aba9-57a7-4ce1-9eac-aa8ffca36ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_26.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7809b9-9ae5-4f53-9eb3-80ab19626831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_27.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af30f9ee-d257-4d11-82e3-663662dc1d19",
   "metadata": {},
   "source": [
    "Scikit-learn provides `HistGradientBoostingClassifier` which is an approximate gradient boosting algorithm similar to `lightgbm` and `xgboost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088c7e5e-3034-46b0-8e25-6dd7cb4baee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_iter=200, max_bins=10)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38df47d0-a3b9-4395-a4cd-7e64fca3210a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e2ad04-4b98-4611-a282-5ddde8980702",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e51089-19c2-4f08-9fb8-7d92b10c2e6a",
   "metadata": {},
   "source": [
    "For gradient-boosting, parameters are coupled, so we cannot set the parameters one after the other anymore. The important parameters are `n_estimators`, `max_depth`, and `learning_rate`.\n",
    "\n",
    "Letâ€™s first discuss the `max_depth` parameter. We saw in the section on gradient-boosting that the algorithm fits the error of the previous tree in the ensemble. Thus, fitting fully grown trees will be detrimental. Indeed, the first tree of the ensemble would perfectly fit (overfit) the data and thus no subsequent tree would be required, since there would be no residuals. Therefore, the tree used in gradient-boosting should have a low depth, typically between 3 to 8 levels. Having very weak learners at each step will help reducing overfitting.\n",
    "\n",
    "With this consideration in mind, the deeper the trees, the faster the residuals will be corrected and less learners are required. Therefore, `n_estimators` should be increased if `max_depth` is lower.\n",
    "\n",
    "Finally, we have overlooked the impact of the `learning_rate` parameter until now. When fitting the residuals, we would like the tree to try to correct all possible errors or only a fraction of them. The learning-rate allows you to control this behaviour. A small learning-rate value would only correct the residuals of very few samples. If a large learning-rate is set (e.g., 1), we would fit the residuals of all samples. So, with a very low learning-rate, we will need more estimators to correct the overall error. However, a too large learning-rate tends to obtain an overfitted ensemble, similar to having a too large tree depth.\n",
    "\n",
    "We will come back in the next chapter how to find the best set of hyperparameters in practice.\n",
    "\n",
    "An option that is useful in histogram gradient boosting is the `early_stopping` parameter. This will split the data internally during `fit` and use a validation set to assess the improvement of adding a new decision tree regressor. If the model detect that adding new estimators will not improve the statistical performance of the model, it will stop the `fit` process. Let's check in practice how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47867c-af64-4866-9b48-56db76acd6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HistGradientBoostingClassifier(early_stopping=True, max_iter=1_000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a8211-4440-457d-b0ac-efdd2e79cf31",
   "metadata": {},
   "source": [
    "We requested 1,000 decision trees that is more than we actually need to fit the data at hand. We can now check the number of trees that has been added in the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72cb181-4dad-4855-877f-e43f45a3320f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5aa5f9-5b9f-461b-bac1-1e22fe607d57",
   "metadata": {},
   "source": [
    "We see that the gradient boosting stopped the learning process after 127 decision trees."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
