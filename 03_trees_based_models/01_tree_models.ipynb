{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c4b3198-7a31-4820-8243-00cc7a6b0b2e",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d903be20-fc27-4096-8a26-2ee06204b8e3",
   "metadata": {},
   "source": [
    "Here we'll explore a class of algorithms based on decision trees.\n",
    "Decision trees at their root are extremely intuitive.  They\n",
    "encode a series of \"if\" and \"else\" choices, similar to how a person might make a decision.\n",
    "However, which questions to ask, and how to proceed for each answer is entirely learned from the data.\n",
    "\n",
    "For example, if you wanted to create a guide to identifying an animal found in nature, you\n",
    "might ask the following series of questions:\n",
    "\n",
    "- Is the animal bigger or smaller than a meter long?\n",
    "    + *bigger*: does the animal have horns?\n",
    "        - *yes*: are the horns longer than ten centimeters?\n",
    "        - *no*: is the animal wearing a collar\n",
    "    + *smaller*: does the animal have two or four legs?\n",
    "        - *two*: does the animal have wings?\n",
    "        - *four*: does the animal have a bushy tail?\n",
    "\n",
    "and so on.  This binary splitting of questions is the essence of a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70310b53-c579-40cc-b344-86c1f4c1ee28",
   "metadata": {},
   "source": [
    "One of the main benefit of tree-based models is that they require little preprocessing of the data.\n",
    "They can work with variables of different types (continuous and discrete) and are invariant to scaling of the features.\n",
    "\n",
    "Another benefit is that tree-based models are what is called \"nonparametric\", which means they don't have a fix set of parameters to learn. Instead, a tree model can become more and more flexible, if given more data.\n",
    "In other words, the number of free parameters grows with the number of samples and is not fixed, as for example in linear models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59290517-9dc3-450e-a725-5ceb278f7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temporary fix to avoid spurious warning raised in scikit-learn 1.0.0\n",
    "# it will be solved in scikit-learn 1.0.1\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"X has feature names.*\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111efaf0-3da3-4adb-a3db-39923d2fe281",
   "metadata": {},
   "source": [
    "## Decision Tree Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d252d208-aba4-4e5b-aee6-db6d2b744de7",
   "metadata": {},
   "source": [
    "### Generate a simple dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e146cee9-d525-4e36-b7e2-c97e2b8337ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(\n",
    "    centers=[[0, 0], [1, 1]], random_state=61526, n_samples=100\n",
    ")\n",
    "X = pd.DataFrame(X, columns=[\"Feature #0\", \"Feature #1\"])\n",
    "class_names = np.array([\"class #0\", \"class #1\"])\n",
    "y = pd.Series(class_names[y], name=\"Classes\").astype(\"category\")\n",
    "data = pd.concat([X, y], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98270d26-6558-4886-88ed-768c635d6ec0",
   "metadata": {},
   "source": [
    "First, let's look at the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c383b3fa-2e14-4523-96d4-8eb28122d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "classes = np.unique(y)\n",
    "print(f\"The class labels are: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170da816-5066-49fb-a535-602374771ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae10ee4-717a-4562-a734-22b9d90add99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_ = data.plot.scatter(\n",
    "    x=\"Feature #0\",\n",
    "    y=\"Feature #1\",\n",
    "    c=\"Classes\",\n",
    "    s=50,\n",
    "    cmap=plt.cm.RdBu,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f1669-4368-40c9-8b2b-b3bfaf19d22e",
   "metadata": {},
   "source": [
    "We will create a function to create this scatter plot by passing 2 variables: `data` and `labels`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a58ba11-14cf-4f25-979c-c219fd2dfe5a",
   "metadata": {},
   "source": [
    "### Train a decision tree classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6569418-c2e7-4728-a869-c8122af29f5f",
   "metadata": {},
   "source": [
    "We can learn a set of binary rule using a portion of the data. Using the rules learned, we will predict on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4269006-d97a-4666-b8db-533d59fcaf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, X, y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefabf3f-0442-4abe-8537-06a748309de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=1)\n",
    "tree.fit(X_train, y_train)\n",
    "pred = tree.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68af23a-f66f-47c2-97fe-d4f18fe9fe66",
   "metadata": {},
   "source": [
    "We can plot the decision boundaries found using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda96d1b-32fd-4cc3-81c9-2ba35edfa144",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.plotting import DecisionBoundaryDisplay\n",
    "\n",
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    tree, X_train, cmap=plt.cm.RdBu_r, alpha=0.5\n",
    ")\n",
    "_ = data_train.plot.scatter(\n",
    "    x=\"Feature #0\",\n",
    "    y=\"Feature #1\",\n",
    "    c=\"Classes\",\n",
    "    s=50,\n",
    "    cmap=plt.cm.RdBu_r,\n",
    "    ax=display.ax_\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d77de6-106d-43be-8807-17c11d1eda61",
   "metadata": {},
   "source": [
    "Similarly, we get the following classification on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8a668-6829-42a1-a232-23c52df91249",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = DecisionBoundaryDisplay.from_estimator(\n",
    "    tree, X_test, cmap=plt.cm.RdBu_r, alpha=0.5\n",
    ")\n",
    "_ = data_test.plot.scatter(\n",
    "    x=\"Feature #0\",\n",
    "    y=\"Feature #1\",\n",
    "    c=\"Classes\",\n",
    "    s=50,\n",
    "    cmap=plt.cm.RdBu_r,\n",
    "    ax=display.ax_\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0f375-3377-45bc-b940-8f3f9842a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "_ = plot_tree(\n",
    "    tree, feature_names=X.columns, class_names=class_names, filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41998fd-dba4-4f21-97e7-c7ed24ee578a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "     <li> Modify the depth of the tree and see how the partitioning evolves. </li>\n",
    "     <li>What can you say about under- and over-fitting of the tree model?</li>\n",
    "     <li>How would you choose the best depth?</li>\n",
    "     </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbebbd64-dd7b-4827-aa8a-058684d7f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_01.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9fda5-219d-4848-b989-bbfa718fa8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_02.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5560046f-8db6-4d9b-9bce-58a618392be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_04.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018df0cd-9bbc-4d76-a6e8-1ea0ec279069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_05.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb16c8-eeb8-4be5-839e-327efa112405",
   "metadata": {},
   "source": [
    "There are many parameter that control the complexity of a tree, but the one that might be easiest to understand is the maximum depth. This limits how finely the tree can partition the input space, or how many \"if-else\" questions can be asked before deciding which class a sample lies in.\n",
    "\n",
    "This parameter is important to tune for trees and tree-based models. The interactive plot below shows how underfit and overfit looks like for this model. Having a ``max_depth`` of 1 is clearly an underfit model, while a depth of 7 or 8 clearly overfits. The maximum depth a tree can be grown at for this dataset is 8, at which point each leave only contains samples from a single class. This is known as all leaves being \"pure.\"\n",
    "\n",
    "In the interactive plot below, the regions are assigned blue and red colors to indicate the predicted class for that region. The shade of the color indicates the predicted probability for that class (darker = higher probability), while yellow regions indicate an equal predicted probability for either class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff283f55-03e3-4924-8d42-5cd66a6f0677",
   "metadata": {},
   "source": [
    "### Aside note regarding the partitioning in decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da27159-f788-4a93-a48f-1dc09fe60c50",
   "metadata": {},
   "source": [
    "In this section, we will go slightly more into details regading how a tree is selecting the best partition. First, instead of using synthetic data, we will use a real dataset this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a382c562-4867-459d-bd11-116773518d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"../datasets/penguins.csv\")\n",
    "dataset = dataset.dropna(subset=[\"Body Mass (g)\"])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65765768-2eec-4911-b086-720df4aa842f",
   "metadata": {},
   "source": [
    "We will build a decision tree to classify the penguin species using their body mass as a feature. To simplify the problem will focus only the Adelie and Gentoo species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5959bcfb-7cb3-4b51-999a-84f45e401ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only select the column of interest\n",
    "dataset = dataset[[\"Body Mass (g)\", \"Species\"]]\n",
    "# Make the species name more readable\n",
    "dataset[\"Species\"] = dataset[\"Species\"].apply(lambda x: x.split()[0])\n",
    "# Only select the Adelie and Gentoo penguins\n",
    "dataset = dataset.set_index(\"Species\").loc[[\"Adelie\", \"Gentoo\"], :]\n",
    "# Sort all penguins by their body mass\n",
    "dataset = dataset.sort_values(by=\"Body Mass (g)\")\n",
    "# Convert the dataframe (2D) to a series (1D)\n",
    "dataset = dataset.squeeze()\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306bfb03-3e68-4eab-b899-f0c3d187952b",
   "metadata": {},
   "source": [
    "We will first look at the body mass distribution for each specie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bf5d23-cbf4-42b7-8751-e4d3e9edb6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "dataset.groupby(\"Species\").plot.hist(ax=ax, alpha=0.7, legend=True)\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "_ = ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c7fc06-7971-4fbc-b629-bc489720478f",
   "metadata": {},
   "source": [
    "Instead to look at the distribution, we can look at all samples directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a1d53c-c8c1-4ea5-b0ed-39563cf9c667",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.swarmplot(x=dataset.values, y=[\"\"] * len(dataset), hue=dataset.index)\n",
    "ax.set_xlabel(dataset.name)\n",
    "_ = ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f7c32-b6b3-4132-8115-444fe4ee9558",
   "metadata": {},
   "source": [
    "When we build a tree, we want to find splits, one at the time, such that we partition the data in way that classes as \"unmixed\" as possible. Let's make a first completely random split to highlight the principle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555826a2-6f83-4796-9ecf-9e330a3b7626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random state such we all have the same results\n",
    "rng = np.random.RandomState(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c71e7aa-9356-4c33-b056-8353d7526f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = rng.choice(dataset.size)\n",
    "\n",
    "ax = sns.swarmplot(x=dataset.values, y=[\"\"] * len(dataset), hue=dataset.index)\n",
    "ax.set_xlabel(dataset.name)\n",
    "ax.set_title(f\"Body mass threshold: {dataset[random_idx]} grams\")\n",
    "ax.vlines(dataset[random_idx], -1, 1, color=\"red\", linestyle=\"--\")\n",
    "_ = ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b89474-6a1d-46e2-9628-ad37908f26d1",
   "metadata": {},
   "source": [
    "Once the split done, we seek for having two partitions for which the samples are as much as possible from a single class and contains as many samples as possible. In decision tree, we used a **criterion** to assess the quality of a split. The **entropy** is one of the statistic which can describe the class mixity in a partition. Let's compute the entropy for the full dataset, the set on the left of the threshold and the set on the right of the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd0568-2279-4aaa-958a-6857a2f8a85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4752a3ce-1907-4ab5-9a06-08794914bb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb01a74-627d-4a49-affc-1bd77416f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_entropy = entropy(\n",
    "    dataset.index.value_counts(normalize=True)\n",
    ")\n",
    "parent_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29002d5e-cd94-4f5e-8179-31cdd22a71a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_entropy = entropy(\n",
    "    dataset[:random_idx].index.value_counts(normalize=True)\n",
    ")\n",
    "left_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f7e6ea-73ab-4b4a-9d86-e440d5e6018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_entropy = entropy(\n",
    "    dataset[random_idx:].index.value_counts(normalize=True)\n",
    ")\n",
    "right_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26c388-0b5b-4cf6-bd0a-78c3646bfa53",
   "metadata": {},
   "source": [
    "We can see the quality of the split by combining the entropies. This is known as the **information gain**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9d33f-31fc-4e2b-9279-4cff630a6356",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_entropy - (left_entropy + right_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47769b8b-0b35-4129-a305-89498d4f9d77",
   "metadata": {},
   "source": [
    "However, we should normalize the entropies with the number of samples in each sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fc4c04-e82b-4287-870b-f3c0e546739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(labels_parent, labels_left, labels_right):\n",
    "    # compute the entropies\n",
    "    entropy_parent = entropy(labels_parent.value_counts(normalize=True))\n",
    "    entropy_left = entropy(labels_left.value_counts(normalize=True))\n",
    "    entropy_right = entropy(labels_right.value_counts(normalize=True))\n",
    "\n",
    "    n_samples_parent = labels_parent.size\n",
    "    n_samples_left = labels_left.size\n",
    "    n_samples_right = labels_right.size\n",
    "\n",
    "    # normalize with the number of samples\n",
    "    normalized_entropy_left = ((n_samples_left / n_samples_parent) * \n",
    "                               entropy_left)\n",
    "    normalized_entropy_right = ((n_samples_right / n_samples_parent) *\n",
    "                                entropy_right)\n",
    "\n",
    "    return (entropy_parent -\n",
    "            normalized_entropy_left - normalized_entropy_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc9947-f2fa-4dab-957e-a8c2c8d2e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "information_gain(\n",
    "    dataset.index,\n",
    "    dataset[:random_idx].index,\n",
    "    dataset[random_idx:].index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80449f2-344c-4efe-b03f-fabc19175948",
   "metadata": {},
   "source": [
    "So, we can compute the information gain for all possible body mass thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c6b715-d21e-419f-a0b3-8890b3c073c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_information_gain = pd.Series(\n",
    "    [information_gain(dataset.index, dataset[:idx].index, dataset[idx:].index)\n",
    "     for idx in range(dataset.size)],\n",
    "    index=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f1d82f-80c5-4df6-8612-ce12fba240fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = all_information_gain.plot()\n",
    "_ = ax.set_ylabel(\"Information gain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b00e3b-f68c-4c53-9c7d-68e25c571f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (all_information_gain * -1).plot(color=\"red\", label=\"Information gain\")\n",
    "ax = sns.swarmplot(x=dataset.values, y=[\"\"] * len(dataset), hue=dataset.index)\n",
    "_ = ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7805e0db-3872-43ba-9958-f4d4fc2ea7b1",
   "metadata": {},
   "source": [
    "We can see that the maximum of the information gain corresponds to the split which best partition our data. So we can check the corresponding body mass threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f3a309-b2c6-4878-b9e9-41a316a297e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_information_gain.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d09398-7c57-49e6-b7f8-1c067d010acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = (all_information_gain * -1).plot(color=\"red\", label=\"Information gain\")\n",
    "ax = sns.swarmplot(x=dataset.values, y=[\"\"] * len(dataset), hue=dataset.index)\n",
    "ax.vlines(\n",
    "    all_information_gain.idxmax(), -1, 1,\n",
    "    color=\"red\", linestyle=\"--\"\n",
    ")\n",
    "_ = ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f849f6e5-b87d-43a8-aac0-9a8f3984224d",
   "metadata": {},
   "source": [
    "## Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a10ee-cb7d-4bb0-9341-412e13f41a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(42)\n",
    "x = np.linspace(-3, 3, 100)\n",
    "y_no_noise = np.sin(4 * x) + x\n",
    "y = y_no_noise + rnd.normal(size=len(x))\n",
    "X = x.reshape(-1, 1)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Feature X')\n",
    "plt.ylabel('Target y')\n",
    "_ = plt.scatter(X, y, s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edf1f90-98ae-4f87-a2a5-ff9230b5cd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "reg = DecisionTreeRegressor(max_depth=2)\n",
    "reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb3973-a0a0-4a38-8faa-d3728d1fbaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.linspace(-3, 3, 1000).reshape((-1, 1))\n",
    "y_test = reg.predict(X_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(X_test.ravel(), y_test, color='tab:blue', label=\"prediction\")\n",
    "plt.plot(X.ravel(), y, 'C7.', label=\"training data\")\n",
    "_ = plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3442af7a-0c6f-4a22-96ca-875b19e7c2a1",
   "metadata": {},
   "source": [
    "A single decision tree allows us to estimate the signal in a non-parametric way,\n",
    "but clearly has some issues.  In some regions, the model shows high bias and\n",
    "under-fits the data.\n",
    "(seen in the long flat lines which don't follow the contours of the data),\n",
    "while in other regions the model shows high variance and over-fits the data\n",
    "(reflected in the narrow spikes which are influenced by noise in single points)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf164d1-d778-4413-9e9f-bffee29b5aa7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>EXERCISE</b>:\n",
    "     <ul>\n",
    "      <li>\n",
    "      Take the above example and repeat the training/testing by changing depth of the tree.\n",
    "      </li>\n",
    "      <li>\n",
    "      What can you conclude?\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f25da-ee9c-48a2-8b00-319e4fa905ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec783695-1dd3-4e74-8400-ef67fbc55684",
   "metadata": {},
   "source": [
    "## Other tree hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b42f3e-be17-4b5e-98be-1c29816620bc",
   "metadata": {},
   "source": [
    "The max_depth hyperparameter controls the overall complexity of the tree. This parameter is adequate under the assumption that a tree is built is symmetric. However, there is no guarantee that a tree will be symmetric. Indeed, optimal generalization performance could be reached by growing some of the branches deeper than some others.\n",
    "\n",
    "We will built a dataset where we will illustrate this asymmetry. We will generate a dataset composed of 2 subsets: one subset where a clear separation should be found by the tree and another subset where samples from both classes will be mixed. It implies that a decision tree will need more splits to classify properly samples from the second subset than from the first subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bfa196-c48d-477c-b317-6d294902f124",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "feature_names = [\"Feature #0\", \"Feature #1\"]\n",
    "target_name = \"Class\"\n",
    "\n",
    "# Blobs that will be interlaced\n",
    "X_1, y_1 = make_blobs(\n",
    "    n_samples=300, centers=[[0, 0], [-1, -1]], random_state=0\n",
    ")\n",
    "# Blobs that will be easily separated\n",
    "X_2, y_2 = make_blobs(\n",
    "    n_samples=300, centers=[[3, 6], [7, 0]], random_state=0\n",
    ")\n",
    "\n",
    "X = np.concatenate([X_1, X_2], axis=0)\n",
    "y = np.concatenate([y_1, y_2])\n",
    "data = np.concatenate([X, y[:, np.newaxis]], axis=1)\n",
    "data = pd.DataFrame(\n",
    "    data, columns=feature_names + [target_name])\n",
    "data[target_name] = data[target_name].astype(np.int64).astype(\"category\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a38614-9dd9-4455-b881-6e1e457cce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10, 8))\n",
    "data.plot.scatter(\n",
    "    x=\"Feature #0\",\n",
    "    y=\"Feature #1\",\n",
    "    c=\"Class\",\n",
    "    s=100,\n",
    "    cmap=plt.cm.RdBu,\n",
    "    edgecolor=\"k\",\n",
    "    ax=ax,\n",
    ")\n",
    "_ = plt.title(\"Synthetic dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7254605b-3843-4917-a16d-60ce656f30fd",
   "metadata": {},
   "source": [
    "We will first train a shallow decision tree with max_depth=2. We would expect this depth to be enough to separate the blobs that are easy to separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c581c952-e39c-4926-9c15-cd37d4e96f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 2\n",
    "tree = DecisionTreeClassifier(max_depth=max_depth)\n",
    "tree.fit(X, y)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10, 8))\n",
    "DecisionBoundaryDisplay.from_estimator(tree, X, cmap=plt.cm.RdBu, ax=ax)\n",
    "data.plot.scatter(\n",
    "    x=\"Feature #0\",\n",
    "    y=\"Feature #1\",\n",
    "    c=\"Class\",\n",
    "    s=100,\n",
    "    cmap=plt.cm.RdBu,\n",
    "    edgecolor=\"k\",\n",
    "    ax=ax,\n",
    ")\n",
    "_ = plt.title(f\"Decision tree with max-depth of {max_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f86ca1-8ac6-45de-a78e-90a2f352faca",
   "metadata": {},
   "source": [
    "As expected, we see that the blue blob on the right and the red blob on the top are easily separated. However, more splits will be required to better split the blob were both blue and red data points are mixed.\n",
    "\n",
    "Indeed, we see that red blob on the top and the blue blob on the right of the plot are perfectly separated. However, the tree is still making mistakes in the area where the blobs are mixed together. Letâ€™s check the tree representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec47e27-6f41-4e74-8a8b-7bd9627b3df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(15, 8))\n",
    "_ = plot_tree(\n",
    "    tree, feature_names=feature_names, class_names=class_names, filled=True, ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349062a6-4fda-426f-b5c5-fae9dada8806",
   "metadata": {},
   "source": [
    "We see that the right branch achieves perfect classification. Now, we increase the depth to check how the tree will grow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2cb47f-71cc-479f-9f60-a2f23845321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = 6\n",
    "tree = DecisionTreeClassifier(max_depth=max_depth)\n",
    "tree.fit(X, y)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10, 8))\n",
    "DecisionBoundaryDisplay.from_estimator(tree, X, cmap=plt.cm.RdBu, ax=ax)\n",
    "data.plot.scatter(\n",
    "    x=\"Feature #0\",\n",
    "    y=\"Feature #1\",\n",
    "    c=\"Class\",\n",
    "    s=100,\n",
    "    cmap=plt.cm.RdBu,\n",
    "    edgecolor=\"k\",\n",
    "    ax=ax,\n",
    ")\n",
    "_ = plt.title(f\"Decision tree with max-depth of {max_depth}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb66cb1-a9f8-49be-930b-6a0cd14a029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(25, 15))\n",
    "_ = plot_tree(\n",
    "    tree, feature_names=feature_names, class_names=class_names, filled=True, ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8587e1-958e-4ce1-85ea-9511b080ca4d",
   "metadata": {},
   "source": [
    "As expected, the left branch of the tree continue to grow while no further splits were done on the right branch. Fixing the max_depth parameter would cut the tree horizontally at a specific level, whether or not it would be more beneficial that a branch continue growing.\n",
    "\n",
    "The hyperparameters min_samples_leaf, min_samples_split, max_leaf_nodes, or min_impurity_decrease allows growing asymmetric trees and apply a constraint at the leaves or nodes level. We will check the effect of min_samples_leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462deee-a1f2-48e8-806e-0606eefdf54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_leaf = 20\n",
    "tree = DecisionTreeClassifier(min_samples_leaf=min_samples_leaf)\n",
    "tree.fit(X, y)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10, 8))\n",
    "DecisionBoundaryDisplay.from_estimator(tree, X, cmap=plt.cm.RdBu, ax=ax)\n",
    "data.plot.scatter(\n",
    "    x=\"Feature #0\",\n",
    "    y=\"Feature #1\",\n",
    "    c=\"Class\",\n",
    "    s=100,\n",
    "    cmap=plt.cm.RdBu,\n",
    "    edgecolor=\"k\",\n",
    "    ax=ax,\n",
    ")\n",
    "_ = plt.title(\n",
    "    f\"Decision tree with leaf having at least {min_samples_leaf} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ca587-e84b-481d-88c2-03c703196d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(15, 15))\n",
    "_ = plot_tree(\n",
    "    tree, feature_names=feature_names, class_names=class_names, filled=True, ax=ax\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcd1a35-0ffc-431b-b6c3-67574bd2f08d",
   "metadata": {},
   "source": [
    "This hyperparameter allows to have leaves with a minimum number of samples and no further splits will be search otherwise. Therefore, these hyperparameters could be an alternative to fix the max_depth hyperparameter."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
