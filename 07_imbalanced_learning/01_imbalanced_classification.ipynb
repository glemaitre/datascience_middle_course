{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with imbalanced dataset\n",
    "\n",
    "This notebook will tackle the problem known as classification with imbalanced classes. We will first introduce the problem and emphasize the difficulties related to both training and evaluating a predictive model under these circumstances.\n",
    "\n",
    "Let's start to fetch a dataset from OpenML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "dataset = fetch_openml(data_id=42397, as_frame=True)\n",
    "X, y = dataset.data, dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ID does not give us too much information regarding this dataset. Let's get some information looking at the related description provided by OpenML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we got a bit more information. There is three important information: (i)the dataset is a classification problem to detect credit card frauds; (ii) it is supposidely highly imbalanced; (iii) the features are numerical features resulting from a principal component analysis (PCA) decomposition. Since we don't have a clue regarding the number of original features, we only know that the features `V**` are a linear combination of the original features. Such processing is used to encode the original data but let us the possibility to still work with a surrogate.\n",
    "\n",
    "Let's have a first look at the dataset then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in addition of the PCA features, we also have two other features: `\"Time\"` and `\"Amount\"` corresponding to the relative time of the transaction and the amount of the transaction, respectively. We can also check the size of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have almost 300,000 available samples. Let's now have a look at our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is a binary target: `True` indicates that the transaction was a fraud while `False` indicates that it as legitimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before to start training an powerful predictive model, it is always nice to start by having a baseline. Earlier in this course, we notably presented two approaches to get baselines that we expect to be beaten by any predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "        <li>Create a dummy predictor that will always predict the most frequent class of the training set.</li>\n",
    "        <li>Use cross-validation to get an estimate of the test score of such dummy baseline.</li>\n",
    "        <li>Use the accuracy score as an evaluation metric.</li>\n",
    "    </ul>\n",
    "    What can you say about the statistical performance of the model?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# %load solutions/solution_01.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks wonderful. We have a model that is highly accurate. Too accurate to be true. It might be a good idea to have a look at the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "        <li>Split the original data to get a training and a testing set.</li>\n",
    "        <li>Train the previous dummy classifier on the training data.</li>\n",
    "        <li>Plot the confusion matrix using <tt>ConfusionMatrixDisplay.from_estimator</tt>.</li>\n",
    "    </ul>\n",
    "    What can you conclude?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_02.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty logical indeed. We force our estimator to always predict that there is no fraud. However, since there is a lot of legitimate transactions in regards to the fraudulent transactions, computing the accuracy score will not be helpful at representing a metric answering to the question \"How good my predictive model is at detecting credit card fraud?\".\n",
    "\n",
    "Indeed, considering that our \"positive\" outcomes is detecting frauds, we should use metrics that focuses only on the frauds outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics to use in imbalanced classification setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before to look at the impact of imbalanced classes on the model, we can first define the metrics that we should use in this setting. It will help to compare models later.\n",
    "\n",
    "As mentioned earlier, we should use metrics that only focus on the \"positive\" outcome. Thus, looking at the metrics derived from the confusion matrix, we could be interested in the following:\n",
    "\n",
    "- recall (also called sensitivity)\n",
    "- precision\n",
    "- average precision (area under the curve of the precision-recall curve)\n",
    "- balanced accuracy\n",
    "\n",
    "Let's see what these metrics would have give us as indication regarding the statistical performance of our dummy predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "        <li>Repeat the cross-validation evaluation by passing the above scores as metrics to be evaluated.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_04.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that all scores are reflecting that our model is not good at detecting credit card fraud. Now to have we have a set of sensible metrics, we can go ahead looking at the impact of training a model on an imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of imbalanced classes on the training process\n",
    "\n",
    "In the remainder of this notebook, we will compare the impact of imbalanced classes on the training process and a couple of strategies allowing to improve and alleviate this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "index = []\n",
    "scores = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scores(scores, cv_results):\n",
    "    for key in cv_results:\n",
    "        prefix = \"test_\"\n",
    "        if prefix in key:\n",
    "            scores[key.replace(prefix, \"\").replace(\"_\", \" \").capitalize()].append(\n",
    "                cv_results[key].mean()\n",
    "            )\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy baseline\n",
    "\n",
    "Let's store the results of the dummy baseline that we used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DummyClassifier(strategy=\"most_frequent\")\n",
    "cv_results = cross_validate(classifier, X, y, scoring=scoring, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.append(classifier.__class__.__name__)\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear classifier baseline\n",
    "\n",
    "Now, we will use a linear classifier that is `LogisticRegression` with the default parameter. It will serve us as a baseline to compare future linear predictive models.\n",
    "As already presented, we will normalize the feature using a `StandardScaler` that is a good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "classifier = make_pipeline(\n",
    "    StandardScaler(), LogisticRegression(max_iter=1000)\n",
    ")\n",
    "cv_results = cross_validate(classifier, X, y, scoring=scoring, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.append(classifier[-1].__class__.__name__)\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that our model is indeed learning something. It is much better than the baseline model. However, we will see that it is indeed impacted by the class imbalance and it can do even better.\n",
    "\n",
    "Now, we will also train a `RandomForestClassifier` in order to have a powerful tree-based model for later comparison as well. `RandomForestClassifier` will not require any preprocessing since we are only dealing with numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_jobs=-1)\n",
    "cv_results = cross_validate(classifier, X, y, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.append(classifier.__class__.__name__)\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that `RandomForestClassifier` is also learning something from data. The average precision is lower than for the linear model but the recall is higher.\n",
    "\n",
    "However, we will see that both models can do both better in terms of metrics by tweaking the training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction of `sample_weight`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we presented boosting algorithm where we used `sample_weight` to tweak the training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    Come with a strategy that would use <tt>sample_weight</tt> at <tt>fit</tt> in order to alleviate the issue of class imbalanced. Feel free to use a single split instead of cross-validation to evaluate your approach at first.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_05.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_06.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_07.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_08.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `sample_weight` is providing the flexibility to change any weight for a given sample, scikit-learn provides sometimes a `class_weight` attribute in some estimator that would implement some strategie to reweight samples of the different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `class_weight` instead of `sample_weight`\n",
    "\n",
    "Most of the models in `scikit-learn` have a parameter `class_weight`. This\n",
    "parameter will affect the computation of the loss in linear model or the\n",
    "criterion in the tree-based model to penalize differently a false\n",
    "classification from the minority and majority class. We can set\n",
    "`class_weight=\"balanced\"` such that the weight applied is inversely\n",
    "proportional to the class frequency. We test this parametrization in both\n",
    "linear model and tree-based model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(class_weight=\"balanced\", max_iter=1000),\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=scoring, n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.append(f\"{classifier[-1].__class__.__name__} with balanced class weight\")\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this parameter has an impact on the overall performance. Looking at the precision and recall, we observe that our model becomes sensitive (it detects most of the fraud) at the cost of false detection. Since the balanced accuracy is an average of the recall of each class, the metric is still high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(class_weight=\"balanced\")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=scoring, n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "index.append(f\"{classifier.__class__.__name__} with balanced class weight\")\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can as well see an impact of setting this parameter with `RandomForestClassifier`. With this model, the weights will increase the sensitivity of the model (i.e. increased recall) but with no trade-off on the precision.\n",
    "\n",
    "An intuition regarding this results and difference with the `LogisticRegression` might be due to the fact that the model is non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling instead of passing weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that the semantic of `sample_weight` would be the following: a weight of 0 will mean that we don't consider the sample while a weight of 2 will be equivalent of having twice the sample in the dataset.\n",
    "\n",
    "In the case that a model is not providing `sample_weight` and `class_weight` another library called `imbalanced-learn` allows to use an arbritrary resampling strategy in a pipeline. We will use these strategy to show that they are pretty much equivalent to `sample_weight` or `class_weight`. However, they would allow to specify a specific balancing ratio and could even be find by grid-search. \n",
    "\n",
    "Note that we are importing `make_pipeline` from `imblearn` because the `Pipeline` from `scikit-learn` will not handle sampler from `imbalanced-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline as make_pipeline_with_sampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "classifier = make_pipeline_with_sampler(\n",
    "    StandardScaler(),\n",
    "    RandomUnderSampler(random_state=42),\n",
    "    LogisticRegression(max_iter=1000),\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=scoring, n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "index.append(f\"{classifier[-1].__class__.__name__} with {classifier[-2].__class__.__name__}\")\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    Repeat the experiment above but try to fine tune the balancing ratio by grid-search. Optimize the average precision score. The parameter to tune is called <tt>samling_strategy</tt>. You can refert to the <a href=\"https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\">documentation</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_09.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_10.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.append(\"LogisticRegression with RandomUnderSampler with an optimal ratio\")\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the previous experiment for the `RandomForestClassifier` and observe the impact of the sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"randomundersampler__sampling_strategy\": np.logspace(-2, 0, num=15)\n",
    "}\n",
    "classifier = GridSearchCV(\n",
    "    make_pipeline_with_sampler(\n",
    "        RandomUnderSampler(random_state=42),\n",
    "        RandomForestClassifier(n_jobs=-1),\n",
    "    ),\n",
    "    param_grid=param_grid,\n",
    "    scoring=make_scorer(\n",
    "        average_precision_score, needs_proba=True, pos_label=\"True\"\n",
    "    ),\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=scoring, return_estimator=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for estimator in cv_results[\"estimator\"]:\n",
    "    print(estimator.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "index.append(\"RandomForestClassifier with RandomUnderSampler with an optimal ratio\")\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating sampling within ensemble methods\n",
    "\n",
    "Some methods based on ensemble are integrating some inner resampling that lead to more efficient algorithms. There are notably two algorithms. Let's start to show `BalancedRandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "classifier = BalancedRandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=scoring,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the resampling happen at the level of the bootstrap, each tree in the forest is created on a lower number of samples. It will lower the computational cost. Resampling each bootstrap will also allow to potentially see more of the original data than with a strategy that resample the full training set before hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "index.append(classifier.__class__.__name__)\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is as well possible to fine the ratio of the internal resampling as we previously did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"sampling_strategy\": np.logspace(-2, 0, num=10)\n",
    "}\n",
    "classifier = GridSearchCV(\n",
    "    BalancedRandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid=param_grid,\n",
    "    scoring=make_scorer(\n",
    "        average_precision_score, needs_proba=True, pos_label=\"True\"\n",
    "    ),\n",
    "    \n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=scoring,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.append(\"BalancedRandomForestClassifier with optimal ratio\")\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition of the `BalancedRandomForestClassifier`, `imbalanced-learn` provides a `BalancedBaggingClassifier` that accepts any kind of estimator. Each estimator will be trained on a resampled bootstrap. Here, we show that we could use a strong learner like an `HistGradientBoostingClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "param_grid = {\n",
    "    \"sampling_strategy\": np.logspace(-2.1, 0, num=10)\n",
    "}\n",
    "classifier = GridSearchCV(\n",
    "    BalancedBaggingClassifier(\n",
    "        base_estimator=HistGradientBoostingClassifier(max_iter=1_000, early_stopping=True, random_state=42),\n",
    "        n_estimators=5,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    param_grid=param_grid,\n",
    "    scoring=make_scorer(\n",
    "        average_precision_score, needs_proba=True, pos_label=\"True\"\n",
    "    ),\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=scoring, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.append(\"BalancedBaggingClassifier with optimal ratio\")\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last approach is probably the most effective but request a huge amount of resource since it relies on powerful models.\n",
    "\n",
    "However keep in mind that whatever we did here was to optimize a given metric. Is the metric choosen the right one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost-sensitive metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with a business oriented dataset, it might be interested to ask ourselve if the metrics chosen to optimized our model previously were also meaningful for our application (business).\n",
    "\n",
    "Let's define a real cost-driven metric based on the confusion matrix. We will compute the confusion matrix given us the true positive and negative and the false positive en negative. We will then apply some business rules (completely arbitrary) to convert it into a monetary cost/benefit metric.\n",
    "\n",
    "In short, we could have the following rules:\n",
    "\n",
    "- not detecting a fraud will cost us the amount of the transaction\n",
    "- detecting a fraud will benefit us 20 euros\n",
    "- refusing a legitimate transaction will annoy our customer and cost us 20 euros\n",
    "- accepting a legitimate transaction will increase customer confidence and the benefit will depend of the transaction amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benefit_matrix(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    tp = (y == \"True\") & (y == y_pred)\n",
    "    tn = (y == \"False\") & (y == y_pred)\n",
    "    fp = (y_pred == \"True\") & (y != y_pred)\n",
    "    fn = (y_pred == \"False\") & (y != y_pred)\n",
    "    \n",
    "    # transform into benefit matrix\n",
    "    # little benefit when accepting a true transaction\n",
    "    # it will be related to the amount\n",
    "    tn_benefit = (X[\"Amount\"][tn] * 0.02).sum()\n",
    "    # detecting a fraud is not trivial and arbritary\n",
    "    tp_benefit = tp.sum() * 20\n",
    "    # blocking a legitimate transaction will annoy our\n",
    "    # customer\n",
    "    fp_benefit = fp.sum() * -20\n",
    "    # not blocking a fraud will cost us the transaction\n",
    "    # money\n",
    "    fn_benefit = -(X[\"Amount\"][fn]).sum()\n",
    "    return {\n",
    "        \"tp_benefit\": tp_benefit,\n",
    "        \"tn_benefit\": tn_benefit,\n",
    "        \"fp_benefit\": fp_benefit,\n",
    "        \"fn_benefit\": fn_benefit,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have our business metric, we can evalutate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression()\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    model, X, y, scoring=benefit_matrix, n_jobs=-1, error_score=\"raise\"\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = [name for name in cv_results.columns if \"test_\" in name]\n",
    "cv_results[metric_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[metric_names].sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try to evalute a model where we will resample the dataset. To select the sampling rate, we will maximize the total benefit instead of the average precision that we earlier used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_benefit(estimator, X, y):\n",
    "    return sum(benefit_matrix(estimator, X, y).values())\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"randomundersampler__sampling_strategy\": np.logspace(-2.1, -1, num=15)\n",
    "}\n",
    "classifier = GridSearchCV(\n",
    "    make_pipeline_with_sampler(\n",
    "        StandardScaler(),\n",
    "        RandomUnderSampler(random_state=42),\n",
    "        LogisticRegression(max_iter=1000),\n",
    "    ),\n",
    "    param_grid=param_grid,\n",
    "    scoring=total_benefit,\n",
    "    n_jobs=-1\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=benefit_matrix, return_estimator=True,\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[metric_names].sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    Repeat the above experiment. However, optimize the balanced accuracy within the grid-search. Is it better or worse than optimising in terms of final business metric?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_11.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_12.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solution_13.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
