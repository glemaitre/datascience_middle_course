{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with imbalanced dataset\n",
    "\n",
    "This notebook will tackle the problem known as classification with imbalanced classes. We will first introduce the problem and emphasize the difficulties related to both training and evaluating a predictive model under these circumstances.\n",
    "\n",
    "Let's start to fetch a dataset from OpenML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "dataset = fetch_openml(data_id=42397, as_frame=True)\n",
    "X, y = dataset.data, dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ID does not give us too much information regarding this dataset. Let's get some information looking at the related description provided by OpenML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Source**: Unknown - Date unknown  \n",
      "**Please cite**: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015  \n",
      "\n",
      "Context\n",
      "It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n",
      "\n",
      "Content\n",
      "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
      "\n",
      "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
      "\n",
      "Inspiration\n",
      "Identify fraudulent credit card transactions.\n",
      "\n",
      "Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n",
      "\n",
      "Acknowledgements\n",
      "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (Universite Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on https://www.researchgate.net/project/Fraud-detection-5 and the page of the DefeatFraud project\n",
      "\n",
      "Please cite the following works:\n",
      "\n",
      "Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
      "\n",
      "Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon\n",
      "\n",
      "Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE\n",
      "\n",
      "Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n",
      "\n",
      "Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-Ael; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier\n",
      "\n",
      "Carcillo, Fabrizio; Le Borgne, Yann-Ael; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing\n",
      "\n",
      "Bertrand Lebichot, Yann-Ael Le Borgne, Liyun He, Frederic Oble, Gianluca Bontempi Deep-Learning Domain Adaptation Techniques for Credit Cards Fraud Detection, INNSBDDL 2019: Recent Advances in Big Data and Deep Learning, pp 78-88, 2019\n",
      "\n",
      "Fabrizio Carcillo, Yann-Ael Le Borgne, Olivier Caelen, Frederic Oble, Gianluca Bontempi Combining Unsupervised and Supervised Learning in Credit Card Fraud Detection Information Sciences, 2019\n",
      "\n",
      "Downloaded from openml.org.\n"
     ]
    }
   ],
   "source": [
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we got a bit more information. There is three important information: (i)the dataset is a classification problem to detect credit card frauds; (ii) it is supposidely highly imbalanced; (iii) the features are numerical features resulting from a principal component analysis (PCA) decomposition. Since we don't have a clue regarding the number of original features, we only know that the features `V**` are a linear combination of the original features. Such processing is used to encode the original data but let us the possibility to still work with a surrogate.\n",
    "\n",
    "Let's have a first look at the dataset then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V20       V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...  0.251412 -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ... -0.069083 -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...  0.524980  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ... -0.208038 -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...  0.408542 -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in addition of the PCA features, we also have two other features: `\"Time\"` and `\"Amount\"` corresponding to the relative time of the transaction and the amount of the transaction, respectively. We can also check the size of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have almost 300,000 available samples. Let's now have a look at our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "Name: Class, dtype: category\n",
       "Categories (2, object): ['True', 'False']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target is a binary target: `True` indicates that the transaction was a fraud while `False` indicates that it as legitimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before to start training an powerful predictive model, it is always nice to start by having a baseline. Earlier in this course, we notably presented two approaches to get baselines that we expect to be beaten by any predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "        <li>Create a dummy predictor that will always predict the most frequent class of the training set.</li>\n",
    "        <li>Use cross-validation to get an estimate of the test score of such dummy baseline.</li>\n",
    "        <li>Use the accuracy score as an evaluation metric.</li>\n",
    "    </ul>\n",
    "    What can you say about the statistical performance of the model?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.104676</td>\n",
       "      <td>0.079832</td>\n",
       "      <td>0.998262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.098947</td>\n",
       "      <td>0.087317</td>\n",
       "      <td>0.998262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.095814</td>\n",
       "      <td>0.087114</td>\n",
       "      <td>0.998280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.108059</td>\n",
       "      <td>0.080233</td>\n",
       "      <td>0.998280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.094599</td>\n",
       "      <td>0.094575</td>\n",
       "      <td>0.998280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_score\n",
       "0  0.104676    0.079832    0.998262\n",
       "1  0.098947    0.087317    0.998262\n",
       "2  0.095814    0.087114    0.998280\n",
       "3  0.108059    0.080233    0.998280\n",
       "4  0.094599    0.094575    0.998280"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_classifier = DummyClassifier(strategy=\"most_frequent\")\n",
    "cv_results = cross_validate(dummy_classifier, X, y, n_jobs=-1)\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks wonderful. We have a model that is highly accurate. Too accurate to be true. It might be a good idea to have a look at the confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "        <li>Split the original data to get a training and a testing set.</li>\n",
    "        <li>Train the previous dummy classifier on the training data.</li>\n",
    "        <li>Plot the confusion matrix using <tt>ConfusionMatrixDisplay.from_estimator</tt>.</li>\n",
    "    </ul>\n",
    "    What can you conclude?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "dummy_classifier.fit(X_train, y_train)\n",
    "display = ConfusionMatrixDisplay.from_estimator(\n",
    "    dummy_classifier, X_test, y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty logical indeed. We force our estimator to always predict that there is no fraud. However, since there is a lot of legitimate transactions in regards to the fraudulent transactions, computing the accuracy score will not be helpful at representing a metric answering to the question \"How good my predictive model is at detecting credit card fraud?\".\n",
    "\n",
    "Indeed, considering that our \"positive\" outcomes is detecting frauds, we should use metrics that focuses only on the frauds outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics to use in imbalanced classification setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before to look at the impact of imbalanced classes on the model, we can first define the metrics that we should use in this setting. It will help to compare models later.\n",
    "\n",
    "As mentioned earlier, we should use metrics that only focus on the \"positive\" outcome. Thus, looking at the metrics derived from the confusion matrix, we could be interested in the following:\n",
    "\n",
    "- recall (also called sensitivity)\n",
    "- precision\n",
    "- average precision (area under the curve of the precision-recall curve)\n",
    "- balanced accuracy\n",
    "\n",
    "Let's see what these metrics would have give us as indication regarding the statistical performance of our dummy predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    <ul>\n",
    "        <li>Repeat the cross-validation evaluation by passing the above scores as metrics to be evaluated.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_balanced_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_average_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.097270</td>\n",
       "      <td>0.763962</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.098135</td>\n",
       "      <td>0.774631</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.111174</td>\n",
       "      <td>0.815311</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.096559</td>\n",
       "      <td>0.763883</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.107455</td>\n",
       "      <td>0.796515</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_balanced_accuracy  test_precision  test_recall  \\\n",
       "0  0.097270    0.763962                     0.5             0.0          0.0   \n",
       "1  0.098135    0.774631                     0.5             0.0          0.0   \n",
       "2  0.111174    0.815311                     0.5             0.0          0.0   \n",
       "3  0.096559    0.763883                     0.5             0.0          0.0   \n",
       "4  0.107455    0.796515                     0.5             0.0          0.0   \n",
       "\n",
       "   test_average_precision  \n",
       "0                0.001738  \n",
       "1                0.001738  \n",
       "2                0.001720  \n",
       "3                0.001720  \n",
       "4                0.001720  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "scoring = {\n",
    "    \"balanced_accuracy\": make_scorer(balanced_accuracy_score),\n",
    "    \"precision\": make_scorer(precision_score, pos_label=\"True\"),\n",
    "    \"recall\": make_scorer(recall_score, pos_label=\"True\"),\n",
    "    \"average_precision\": make_scorer(\n",
    "        average_precision_score, needs_proba=True, pos_label=\"True\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    dummy_classifier, X, y, scoring=scoring, n_jobs=-1\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that all scores are reflecting that our model is not good at detecting credit card fraud. Now to have we have a set of sensible metrics, we can go ahead looking at the impact of training a model on an imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of imbalanced classes on the training process\n",
    "\n",
    "In the remainder of this notebook, we will compare the impact of imbalanced classes on the training process and a couple of strategies allowing to improve and alleviate this issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "index = []\n",
    "scores = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_scores(scores, cv_results):\n",
    "    for key in cv_results:\n",
    "        prefix = \"test_\"\n",
    "        if prefix in key:\n",
    "            scores[key.replace(prefix, \"\").replace(\"_\", \" \").capitalize()].append(\n",
    "                cv_results[key].mean()\n",
    "            )\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy baseline\n",
    "\n",
    "Let's store the results of the dummy baseline that we used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DummyClassifier(strategy=\"most_frequent\")\n",
    "cv_results = cross_validate(classifier, X, y, scoring=scoring, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Average precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Balanced accuracy  Precision  Recall  Average precision\n",
       "DummyClassifier                0.5        0.0     0.0           0.001727"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.append(classifier.__class__.__name__)\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear classifier baseline\n",
    "\n",
    "Now, we will use a linear classifier that is `LogisticRegression` with the default parameter. It will serve us as a baseline to compare future linear predictive models.\n",
    "As already presented, we will normalize the feature using a `StandardScaler` that is a good practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "classifier = make_pipeline(\n",
    "    StandardScaler(), LogisticRegression(max_iter=1000)\n",
    ")\n",
    "cv_results = cross_validate(classifier, X, y, scoring=scoring, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Average precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.800512</td>\n",
       "      <td>0.885536</td>\n",
       "      <td>0.601196</td>\n",
       "      <td>0.739402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Balanced accuracy  Precision    Recall  Average precision\n",
       "DummyClassifier              0.500000   0.000000  0.000000           0.001727\n",
       "LogisticRegression           0.800512   0.885536  0.601196           0.739402"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.append(classifier[-1].__class__.__name__)\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that our model is indeed learning something. It is much better than the baseline model. However, we will see that it is indeed impacted by the class imbalance and it can do even better.\n",
    "\n",
    "Now, we will also train a `RandomForestClassifier` in order to have a powerful tree-based model for later comparison as well. `RandomForestClassifier` will not require any preprocessing since we are only dealing with numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_jobs=-1)\n",
    "cv_results = cross_validate(classifier, X, y, scoring=scoring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Average precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.800512</td>\n",
       "      <td>0.885536</td>\n",
       "      <td>0.601196</td>\n",
       "      <td>0.739402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.796670</td>\n",
       "      <td>0.753393</td>\n",
       "      <td>0.790270</td>\n",
       "      <td>0.732701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Balanced accuracy  Precision    Recall  \\\n",
       "DummyClassifier                  0.500000   0.000000  0.000000   \n",
       "LogisticRegression               0.800512   0.885536  0.601196   \n",
       "RandomForestClassifier           0.796670   0.753393  0.790270   \n",
       "\n",
       "                        Average precision  \n",
       "DummyClassifier                  0.001727  \n",
       "LogisticRegression               0.739402  \n",
       "RandomForestClassifier           0.732701  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.append(classifier.__class__.__name__)\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that `RandomForestClassifier` is also learning something from data. The average precision is lower than for the linear model but the recall is higher.\n",
    "\n",
    "However, we will see that both models can do both better in terms of metrics by tweaking the training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction of `sample_weight`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we presented boosting algorithm where we used `sample_weight` to tweak the training procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    Come with a strategy that would use <tt>sample_weight</tt> at <tt>fit</tt> in order to alleviate the issue of class imbalanced. Feel free to use a single split instead of cross-validation to evaluate your approach at first.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    0.998273\n",
       "True     0.001727\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_ratio = y.value_counts(normalize=True)\n",
    "class_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_weight = np.zeros_like(y, dtype=np.float64)\n",
    "sample_weight[y == \"True\"] = class_ratio[\"False\"]\n",
    "sample_weight[y == \"False\"] = class_ratio[\"True\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = make_pipeline(\n",
    "    StandardScaler(), LogisticRegression(max_iter=1000)\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=scoring, n_jobs=-1,\n",
    "    fit_params={\n",
    "        \"logisticregression__sample_weight\": sample_weight\n",
    "    }\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_time                  0.969100\n",
       "score_time                1.131283\n",
       "test_balanced_accuracy    0.930946\n",
       "test_precision            0.065803\n",
       "test_recall               0.892228\n",
       "test_average_precision    0.730541\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `sample_weight` is providing the flexibility to change any weight for a given sample, scikit-learn provides sometimes a `class_weight` attribute in some estimator that would implement some strategie to reweight samples of the different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `class_weight` instead of `sample_weight`\n",
    "\n",
    "Most of the models in `scikit-learn` have a parameter `class_weight`. This\n",
    "parameter will affect the computation of the loss in linear model or the\n",
    "criterion in the tree-based model to penalize differently a false\n",
    "classification from the minority and majority class. We can set\n",
    "`class_weight=\"balanced\"` such that the weight applied is inversely\n",
    "proportional to the class frequency. We test this parametrization in both\n",
    "linear model and tree-based model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression(class_weight=\"balanced\", max_iter=1000),\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=scoring, n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Average precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.800512</td>\n",
       "      <td>0.885536</td>\n",
       "      <td>0.601196</td>\n",
       "      <td>0.739402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.796670</td>\n",
       "      <td>0.753393</td>\n",
       "      <td>0.790270</td>\n",
       "      <td>0.732701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with balanced class weight</th>\n",
       "      <td>0.929240</td>\n",
       "      <td>0.062168</td>\n",
       "      <td>0.892228</td>\n",
       "      <td>0.740864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Balanced accuracy  Precision  \\\n",
       "DummyClassifier                                         0.500000   0.000000   \n",
       "LogisticRegression                                      0.800512   0.885536   \n",
       "RandomForestClassifier                                  0.796670   0.753393   \n",
       "LogisticRegression with balanced class weight           0.929240   0.062168   \n",
       "\n",
       "                                                 Recall  Average precision  \n",
       "DummyClassifier                                0.000000           0.001727  \n",
       "LogisticRegression                             0.601196           0.739402  \n",
       "RandomForestClassifier                         0.790270           0.732701  \n",
       "LogisticRegression with balanced class weight  0.892228           0.740864  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.append(f\"{classifier[-1].__class__.__name__} with balanced class weight\")\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this parameter has an impact on the overall performance. Looking at the precision and recall, we observe that our model becomes sensitive (it detects most of the fraud) at the cost of false detection. Since the balanced accuracy is an average of the recall of each class, the metric is still high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(class_weight=\"balanced\")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=scoring, n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Average precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.800512</td>\n",
       "      <td>0.885536</td>\n",
       "      <td>0.601196</td>\n",
       "      <td>0.739402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.796670</td>\n",
       "      <td>0.753393</td>\n",
       "      <td>0.790270</td>\n",
       "      <td>0.732701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with balanced class weight</th>\n",
       "      <td>0.929240</td>\n",
       "      <td>0.062168</td>\n",
       "      <td>0.892228</td>\n",
       "      <td>0.740864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier with balanced class weight</th>\n",
       "      <td>0.840155</td>\n",
       "      <td>0.903390</td>\n",
       "      <td>0.680478</td>\n",
       "      <td>0.743878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Balanced accuracy  \\\n",
       "DummyClassifier                                             0.500000   \n",
       "LogisticRegression                                          0.800512   \n",
       "RandomForestClassifier                                      0.796670   \n",
       "LogisticRegression with balanced class weight               0.929240   \n",
       "RandomForestClassifier with balanced class weight           0.840155   \n",
       "\n",
       "                                                   Precision    Recall  \\\n",
       "DummyClassifier                                     0.000000  0.000000   \n",
       "LogisticRegression                                  0.885536  0.601196   \n",
       "RandomForestClassifier                              0.753393  0.790270   \n",
       "LogisticRegression with balanced class weight       0.062168  0.892228   \n",
       "RandomForestClassifier with balanced class weight   0.903390  0.680478   \n",
       "\n",
       "                                                   Average precision  \n",
       "DummyClassifier                                             0.001727  \n",
       "LogisticRegression                                          0.739402  \n",
       "RandomForestClassifier                                      0.732701  \n",
       "LogisticRegression with balanced class weight               0.740864  \n",
       "RandomForestClassifier with balanced class weight           0.743878  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.append(f\"{classifier.__class__.__name__} with balanced class weight\")\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can as well see an impact of setting this parameter with `RandomForestClassifier`. With this model, the weights will increase the sensitivity of the model (i.e. increased recall) but with no trade-off on the precision.\n",
    "\n",
    "An intuition regarding this results and difference with the `LogisticRegression` might be due to the fact that the model is non-linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling instead of passing weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that the semantic of `sample_weight` would be the following: a weight of 0 will mean that we don't consider the sample while a weight of 2 will be equivalent of having twice the sample in the dataset.\n",
    "\n",
    "In the case that a model is not providing `sample_weight` and `class_weight` another library called `imbalanced-learn` allows to use an arbritrary resampling strategy in a pipeline. We will use these strategy to show that they are pretty much equivalent to `sample_weight` or `class_weight`. However, they would allow to specify a specific balancing ratio and could even be find by grid-search. \n",
    "\n",
    "Note that we are importing `make_pipeline` from `imblearn` because the `Pipeline` from `scikit-learn` will not handle sampler from `imbalanced-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.pipeline import make_pipeline as make_pipeline_with_sampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "classifier = make_pipeline_with_sampler(\n",
    "    StandardScaler(),\n",
    "    RandomUnderSampler(random_state=42),\n",
    "    LogisticRegression(max_iter=1000),\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=scoring, n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Average precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.800512</td>\n",
       "      <td>0.885536</td>\n",
       "      <td>0.601196</td>\n",
       "      <td>0.739402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.796670</td>\n",
       "      <td>0.753393</td>\n",
       "      <td>0.790270</td>\n",
       "      <td>0.732701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with balanced class weight</th>\n",
       "      <td>0.929240</td>\n",
       "      <td>0.062168</td>\n",
       "      <td>0.892228</td>\n",
       "      <td>0.740864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier with balanced class weight</th>\n",
       "      <td>0.840155</td>\n",
       "      <td>0.903390</td>\n",
       "      <td>0.680478</td>\n",
       "      <td>0.743878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with RandomUnderSampler</th>\n",
       "      <td>0.915563</td>\n",
       "      <td>0.042693</td>\n",
       "      <td>0.898330</td>\n",
       "      <td>0.543483</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Balanced accuracy  \\\n",
       "DummyClassifier                                             0.500000   \n",
       "LogisticRegression                                          0.800512   \n",
       "RandomForestClassifier                                      0.796670   \n",
       "LogisticRegression with balanced class weight               0.929240   \n",
       "RandomForestClassifier with balanced class weight           0.840155   \n",
       "LogisticRegression with RandomUnderSampler                  0.915563   \n",
       "\n",
       "                                                   Precision    Recall  \\\n",
       "DummyClassifier                                     0.000000  0.000000   \n",
       "LogisticRegression                                  0.885536  0.601196   \n",
       "RandomForestClassifier                              0.753393  0.790270   \n",
       "LogisticRegression with balanced class weight       0.062168  0.892228   \n",
       "RandomForestClassifier with balanced class weight   0.903390  0.680478   \n",
       "LogisticRegression with RandomUnderSampler          0.042693  0.898330   \n",
       "\n",
       "                                                   Average precision  \n",
       "DummyClassifier                                             0.001727  \n",
       "LogisticRegression                                          0.739402  \n",
       "RandomForestClassifier                                      0.732701  \n",
       "LogisticRegression with balanced class weight               0.740864  \n",
       "RandomForestClassifier with balanced class weight           0.743878  \n",
       "LogisticRegression with RandomUnderSampler                  0.543483  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.append(f\"{classifier[-1].__class__.__name__} with {classifier[-2].__class__.__name__}\")\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    Repeat the experiment above but try to fine tune the balancing ratio by grid-search. Optimize the average precision score. The parameter to tune is called <tt>samling_strategy</tt>. You can refert to the <a href=\"https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\">documentation</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"randomundersampler__sampling_strategy\": np.logspace(-3, -1, num=15)\n",
    "}\n",
    "classifier = GridSearchCV(\n",
    "    make_pipeline_with_sampler(\n",
    "        StandardScaler(),\n",
    "        RandomUnderSampler(random_state=42),\n",
    "        LogisticRegression(max_iter=1000),\n",
    "    ),\n",
    "    param_grid=param_grid,\n",
    "    scoring=make_scorer(\n",
    "        average_precision_score, needs_proba=True, pos_label=\"True\"\n",
    "    ),\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=scoring, n_jobs=-1,\n",
    "    return_estimator=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'randomundersampler__sampling_strategy': 0.019306977288832496}\n",
      "{'randomundersampler__sampling_strategy': 0.019306977288832496}\n",
      "{'randomundersampler__sampling_strategy': 0.0071968567300115215}\n",
      "{'randomundersampler__sampling_strategy': 0.005179474679231213}\n",
      "{'randomundersampler__sampling_strategy': 0.005179474679231213}\n"
     ]
    }
   ],
   "source": [
    "for estimator in cv_results[\"estimator\"]:\n",
    "    print(estimator.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Average precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.800512</td>\n",
       "      <td>0.885536</td>\n",
       "      <td>0.601196</td>\n",
       "      <td>0.739402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.796670</td>\n",
       "      <td>0.753393</td>\n",
       "      <td>0.790270</td>\n",
       "      <td>0.732701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with balanced class weight</th>\n",
       "      <td>0.929240</td>\n",
       "      <td>0.062168</td>\n",
       "      <td>0.892228</td>\n",
       "      <td>0.740864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier with balanced class weight</th>\n",
       "      <td>0.840155</td>\n",
       "      <td>0.903390</td>\n",
       "      <td>0.680478</td>\n",
       "      <td>0.743878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with RandomUnderSampler</th>\n",
       "      <td>0.915563</td>\n",
       "      <td>0.042693</td>\n",
       "      <td>0.898330</td>\n",
       "      <td>0.543483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with RandomUnderSampler with an optimal ratio</th>\n",
       "      <td>0.885788</td>\n",
       "      <td>0.767201</td>\n",
       "      <td>0.772047</td>\n",
       "      <td>0.753711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Balanced accuracy  \\\n",
       "DummyClassifier                                              0.500000   \n",
       "LogisticRegression                                           0.800512   \n",
       "RandomForestClassifier                                       0.796670   \n",
       "LogisticRegression with balanced class weight                0.929240   \n",
       "RandomForestClassifier with balanced class weight            0.840155   \n",
       "LogisticRegression with RandomUnderSampler                   0.915563   \n",
       "LogisticRegression with RandomUnderSampler with...           0.885788   \n",
       "\n",
       "                                                    Precision    Recall  \\\n",
       "DummyClassifier                                      0.000000  0.000000   \n",
       "LogisticRegression                                   0.885536  0.601196   \n",
       "RandomForestClassifier                               0.753393  0.790270   \n",
       "LogisticRegression with balanced class weight        0.062168  0.892228   \n",
       "RandomForestClassifier with balanced class weight    0.903390  0.680478   \n",
       "LogisticRegression with RandomUnderSampler           0.042693  0.898330   \n",
       "LogisticRegression with RandomUnderSampler with...   0.767201  0.772047   \n",
       "\n",
       "                                                    Average precision  \n",
       "DummyClassifier                                              0.001727  \n",
       "LogisticRegression                                           0.739402  \n",
       "RandomForestClassifier                                       0.732701  \n",
       "LogisticRegression with balanced class weight                0.740864  \n",
       "RandomForestClassifier with balanced class weight            0.743878  \n",
       "LogisticRegression with RandomUnderSampler                   0.543483  \n",
       "LogisticRegression with RandomUnderSampler with...           0.753711  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.append(\"LogisticRegression with RandomUnderSampler with an optimal ratio\")\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat the previous experiment for the `RandomForestClassifier` and observe the impact of the sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"randomundersampler__sampling_strategy\": np.logspace(-2, 0, num=15)\n",
    "}\n",
    "classifier = GridSearchCV(\n",
    "    make_pipeline_with_sampler(\n",
    "        RandomUnderSampler(random_state=42),\n",
    "        RandomForestClassifier(n_jobs=-1),\n",
    "    ),\n",
    "    param_grid=param_grid,\n",
    "    scoring=make_scorer(\n",
    "        average_precision_score, needs_proba=True, pos_label=\"True\"\n",
    "    ),\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=scoring, return_estimator=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'randomundersampler__sampling_strategy': 0.07196856730011521}\n",
      "{'randomundersampler__sampling_strategy': 0.517947467923121}\n",
      "{'randomundersampler__sampling_strategy': 0.013894954943731374}\n",
      "{'randomundersampler__sampling_strategy': 0.013894954943731374}\n",
      "{'randomundersampler__sampling_strategy': 0.013894954943731374}\n"
     ]
    }
   ],
   "source": [
    "for estimator in cv_results[\"estimator\"]:\n",
    "    print(estimator.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Average precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.800512</td>\n",
       "      <td>0.885536</td>\n",
       "      <td>0.601196</td>\n",
       "      <td>0.739402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.796670</td>\n",
       "      <td>0.753393</td>\n",
       "      <td>0.790270</td>\n",
       "      <td>0.732701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with balanced class weight</th>\n",
       "      <td>0.929240</td>\n",
       "      <td>0.062168</td>\n",
       "      <td>0.892228</td>\n",
       "      <td>0.740864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier with balanced class weight</th>\n",
       "      <td>0.840155</td>\n",
       "      <td>0.903390</td>\n",
       "      <td>0.680478</td>\n",
       "      <td>0.743878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with RandomUnderSampler</th>\n",
       "      <td>0.915563</td>\n",
       "      <td>0.042693</td>\n",
       "      <td>0.898330</td>\n",
       "      <td>0.543483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with RandomUnderSampler with an optimal ratio</th>\n",
       "      <td>0.885788</td>\n",
       "      <td>0.767201</td>\n",
       "      <td>0.772047</td>\n",
       "      <td>0.753711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier with RandomUnderSampler with an optimal ratio</th>\n",
       "      <td>0.826648</td>\n",
       "      <td>0.514151</td>\n",
       "      <td>0.851288</td>\n",
       "      <td>0.729608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Balanced accuracy  \\\n",
       "DummyClassifier                                              0.500000   \n",
       "LogisticRegression                                           0.800512   \n",
       "RandomForestClassifier                                       0.796670   \n",
       "LogisticRegression with balanced class weight                0.929240   \n",
       "RandomForestClassifier with balanced class weight            0.840155   \n",
       "LogisticRegression with RandomUnderSampler                   0.915563   \n",
       "LogisticRegression with RandomUnderSampler with...           0.885788   \n",
       "RandomForestClassifier with RandomUnderSampler ...           0.826648   \n",
       "\n",
       "                                                    Precision    Recall  \\\n",
       "DummyClassifier                                      0.000000  0.000000   \n",
       "LogisticRegression                                   0.885536  0.601196   \n",
       "RandomForestClassifier                               0.753393  0.790270   \n",
       "LogisticRegression with balanced class weight        0.062168  0.892228   \n",
       "RandomForestClassifier with balanced class weight    0.903390  0.680478   \n",
       "LogisticRegression with RandomUnderSampler           0.042693  0.898330   \n",
       "LogisticRegression with RandomUnderSampler with...   0.767201  0.772047   \n",
       "RandomForestClassifier with RandomUnderSampler ...   0.514151  0.851288   \n",
       "\n",
       "                                                    Average precision  \n",
       "DummyClassifier                                              0.001727  \n",
       "LogisticRegression                                           0.739402  \n",
       "RandomForestClassifier                                       0.732701  \n",
       "LogisticRegression with balanced class weight                0.740864  \n",
       "RandomForestClassifier with balanced class weight            0.743878  \n",
       "LogisticRegression with RandomUnderSampler                   0.543483  \n",
       "LogisticRegression with RandomUnderSampler with...           0.753711  \n",
       "RandomForestClassifier with RandomUnderSampler ...           0.729608  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.append(\"RandomForestClassifier with RandomUnderSampler with an optimal ratio\")\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating sampling within ensemble methods\n",
    "\n",
    "Some methods based on ensemble are integrating some inner resampling that lead to more efficient algorithms. There are notably two algorithms. Let's start to show `BalancedRandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "classifier = BalancedRandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=scoring,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the resampling happen at the level of the bootstrap, each tree in the forest is created on a lower number of samples. It will lower the computational cost. Resampling each bootstrap will also allow to potentially see more of the original data than with a strategy that resample the full training set before hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Average precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.800512</td>\n",
       "      <td>0.885536</td>\n",
       "      <td>0.601196</td>\n",
       "      <td>0.739402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.796670</td>\n",
       "      <td>0.753393</td>\n",
       "      <td>0.790270</td>\n",
       "      <td>0.732701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with balanced class weight</th>\n",
       "      <td>0.929240</td>\n",
       "      <td>0.062168</td>\n",
       "      <td>0.892228</td>\n",
       "      <td>0.740864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier with balanced class weight</th>\n",
       "      <td>0.840155</td>\n",
       "      <td>0.903390</td>\n",
       "      <td>0.680478</td>\n",
       "      <td>0.743878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with RandomUnderSampler</th>\n",
       "      <td>0.915563</td>\n",
       "      <td>0.042693</td>\n",
       "      <td>0.898330</td>\n",
       "      <td>0.543483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with RandomUnderSampler with an optimal ratio</th>\n",
       "      <td>0.885788</td>\n",
       "      <td>0.767201</td>\n",
       "      <td>0.772047</td>\n",
       "      <td>0.753711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier with RandomUnderSampler with an optimal ratio</th>\n",
       "      <td>0.826648</td>\n",
       "      <td>0.514151</td>\n",
       "      <td>0.851288</td>\n",
       "      <td>0.729608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BalancedRandomForestClassifier</th>\n",
       "      <td>0.921138</td>\n",
       "      <td>0.053662</td>\n",
       "      <td>0.908411</td>\n",
       "      <td>0.761745</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Balanced accuracy  \\\n",
       "DummyClassifier                                              0.500000   \n",
       "LogisticRegression                                           0.800512   \n",
       "RandomForestClassifier                                       0.796670   \n",
       "LogisticRegression with balanced class weight                0.929240   \n",
       "RandomForestClassifier with balanced class weight            0.840155   \n",
       "LogisticRegression with RandomUnderSampler                   0.915563   \n",
       "LogisticRegression with RandomUnderSampler with...           0.885788   \n",
       "RandomForestClassifier with RandomUnderSampler ...           0.826648   \n",
       "BalancedRandomForestClassifier                               0.921138   \n",
       "\n",
       "                                                    Precision    Recall  \\\n",
       "DummyClassifier                                      0.000000  0.000000   \n",
       "LogisticRegression                                   0.885536  0.601196   \n",
       "RandomForestClassifier                               0.753393  0.790270   \n",
       "LogisticRegression with balanced class weight        0.062168  0.892228   \n",
       "RandomForestClassifier with balanced class weight    0.903390  0.680478   \n",
       "LogisticRegression with RandomUnderSampler           0.042693  0.898330   \n",
       "LogisticRegression with RandomUnderSampler with...   0.767201  0.772047   \n",
       "RandomForestClassifier with RandomUnderSampler ...   0.514151  0.851288   \n",
       "BalancedRandomForestClassifier                       0.053662  0.908411   \n",
       "\n",
       "                                                    Average precision  \n",
       "DummyClassifier                                              0.001727  \n",
       "LogisticRegression                                           0.739402  \n",
       "RandomForestClassifier                                       0.732701  \n",
       "LogisticRegression with balanced class weight                0.740864  \n",
       "RandomForestClassifier with balanced class weight            0.743878  \n",
       "LogisticRegression with RandomUnderSampler                   0.543483  \n",
       "LogisticRegression with RandomUnderSampler with...           0.753711  \n",
       "RandomForestClassifier with RandomUnderSampler ...           0.729608  \n",
       "BalancedRandomForestClassifier                               0.761745  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.append(classifier.__class__.__name__)\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is as well possible to fine the ratio of the internal resampling as we previously did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"sampling_strategy\": np.logspace(-2, 0, num=10)\n",
    "}\n",
    "classifier = GridSearchCV(\n",
    "    BalancedRandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid=param_grid,\n",
    "    scoring=make_scorer(\n",
    "        average_precision_score, needs_proba=True, pos_label=\"True\"\n",
    "    ),\n",
    "    \n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=scoring,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Average precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.800512</td>\n",
       "      <td>0.885536</td>\n",
       "      <td>0.601196</td>\n",
       "      <td>0.739402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.796670</td>\n",
       "      <td>0.753393</td>\n",
       "      <td>0.790270</td>\n",
       "      <td>0.732701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with balanced class weight</th>\n",
       "      <td>0.929240</td>\n",
       "      <td>0.062168</td>\n",
       "      <td>0.892228</td>\n",
       "      <td>0.740864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier with balanced class weight</th>\n",
       "      <td>0.840155</td>\n",
       "      <td>0.903390</td>\n",
       "      <td>0.680478</td>\n",
       "      <td>0.743878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with RandomUnderSampler</th>\n",
       "      <td>0.915563</td>\n",
       "      <td>0.042693</td>\n",
       "      <td>0.898330</td>\n",
       "      <td>0.543483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with RandomUnderSampler with an optimal ratio</th>\n",
       "      <td>0.885788</td>\n",
       "      <td>0.767201</td>\n",
       "      <td>0.772047</td>\n",
       "      <td>0.753711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier with RandomUnderSampler with an optimal ratio</th>\n",
       "      <td>0.826648</td>\n",
       "      <td>0.514151</td>\n",
       "      <td>0.851288</td>\n",
       "      <td>0.729608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BalancedRandomForestClassifier</th>\n",
       "      <td>0.921138</td>\n",
       "      <td>0.053662</td>\n",
       "      <td>0.908411</td>\n",
       "      <td>0.761745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BalancedRandomForestClassifier with optimal ratio</th>\n",
       "      <td>0.827981</td>\n",
       "      <td>0.546700</td>\n",
       "      <td>0.851288</td>\n",
       "      <td>0.762562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Balanced accuracy  \\\n",
       "DummyClassifier                                              0.500000   \n",
       "LogisticRegression                                           0.800512   \n",
       "RandomForestClassifier                                       0.796670   \n",
       "LogisticRegression with balanced class weight                0.929240   \n",
       "RandomForestClassifier with balanced class weight            0.840155   \n",
       "LogisticRegression with RandomUnderSampler                   0.915563   \n",
       "LogisticRegression with RandomUnderSampler with...           0.885788   \n",
       "RandomForestClassifier with RandomUnderSampler ...           0.826648   \n",
       "BalancedRandomForestClassifier                               0.921138   \n",
       "BalancedRandomForestClassifier with optimal ratio            0.827981   \n",
       "\n",
       "                                                    Precision    Recall  \\\n",
       "DummyClassifier                                      0.000000  0.000000   \n",
       "LogisticRegression                                   0.885536  0.601196   \n",
       "RandomForestClassifier                               0.753393  0.790270   \n",
       "LogisticRegression with balanced class weight        0.062168  0.892228   \n",
       "RandomForestClassifier with balanced class weight    0.903390  0.680478   \n",
       "LogisticRegression with RandomUnderSampler           0.042693  0.898330   \n",
       "LogisticRegression with RandomUnderSampler with...   0.767201  0.772047   \n",
       "RandomForestClassifier with RandomUnderSampler ...   0.514151  0.851288   \n",
       "BalancedRandomForestClassifier                       0.053662  0.908411   \n",
       "BalancedRandomForestClassifier with optimal ratio    0.546700  0.851288   \n",
       "\n",
       "                                                    Average precision  \n",
       "DummyClassifier                                              0.001727  \n",
       "LogisticRegression                                           0.739402  \n",
       "RandomForestClassifier                                       0.732701  \n",
       "LogisticRegression with balanced class weight                0.740864  \n",
       "RandomForestClassifier with balanced class weight            0.743878  \n",
       "LogisticRegression with RandomUnderSampler                   0.543483  \n",
       "LogisticRegression with RandomUnderSampler with...           0.753711  \n",
       "RandomForestClassifier with RandomUnderSampler ...           0.729608  \n",
       "BalancedRandomForestClassifier                               0.761745  \n",
       "BalancedRandomForestClassifier with optimal ratio            0.762562  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.append(\"BalancedRandomForestClassifier with optimal ratio\")\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition of the `BalancedRandomForestClassifier`, `imbalanced-learn` provides a `BalancedBaggingClassifier` that accepts any kind of estimator. Each estimator will be trained on a resampled bootstrap. Here, we show that we could use a strong learner like an `HistGradientBoostingClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "param_grid = {\n",
    "    \"sampling_strategy\": np.logspace(-2.1, 0, num=10)\n",
    "}\n",
    "classifier = GridSearchCV(\n",
    "    BalancedBaggingClassifier(\n",
    "        base_estimator=HistGradientBoostingClassifier(max_iter=1_000, early_stopping=True, random_state=42),\n",
    "        n_estimators=5,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    param_grid=param_grid,\n",
    "    scoring=make_scorer(\n",
    "        average_precision_score, needs_proba=True, pos_label=\"True\"\n",
    "    ),\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=scoring, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Average precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DummyClassifier</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.800512</td>\n",
       "      <td>0.885536</td>\n",
       "      <td>0.601196</td>\n",
       "      <td>0.739402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.796670</td>\n",
       "      <td>0.753393</td>\n",
       "      <td>0.790270</td>\n",
       "      <td>0.732701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with balanced class weight</th>\n",
       "      <td>0.929240</td>\n",
       "      <td>0.062168</td>\n",
       "      <td>0.892228</td>\n",
       "      <td>0.740864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier with balanced class weight</th>\n",
       "      <td>0.840155</td>\n",
       "      <td>0.903390</td>\n",
       "      <td>0.680478</td>\n",
       "      <td>0.743878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with RandomUnderSampler</th>\n",
       "      <td>0.915563</td>\n",
       "      <td>0.042693</td>\n",
       "      <td>0.898330</td>\n",
       "      <td>0.543483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression with RandomUnderSampler with an optimal ratio</th>\n",
       "      <td>0.885788</td>\n",
       "      <td>0.767201</td>\n",
       "      <td>0.772047</td>\n",
       "      <td>0.753711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier with RandomUnderSampler with an optimal ratio</th>\n",
       "      <td>0.826648</td>\n",
       "      <td>0.514151</td>\n",
       "      <td>0.851288</td>\n",
       "      <td>0.729608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BalancedRandomForestClassifier</th>\n",
       "      <td>0.921138</td>\n",
       "      <td>0.053662</td>\n",
       "      <td>0.908411</td>\n",
       "      <td>0.761745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BalancedRandomForestClassifier with optimal ratio</th>\n",
       "      <td>0.827981</td>\n",
       "      <td>0.546700</td>\n",
       "      <td>0.851288</td>\n",
       "      <td>0.762562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BalancedBaggingClassifier with optimal ratio</th>\n",
       "      <td>0.917623</td>\n",
       "      <td>0.668683</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.774913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Balanced accuracy  \\\n",
       "DummyClassifier                                              0.500000   \n",
       "LogisticRegression                                           0.800512   \n",
       "RandomForestClassifier                                       0.796670   \n",
       "LogisticRegression with balanced class weight                0.929240   \n",
       "RandomForestClassifier with balanced class weight            0.840155   \n",
       "LogisticRegression with RandomUnderSampler                   0.915563   \n",
       "LogisticRegression with RandomUnderSampler with...           0.885788   \n",
       "RandomForestClassifier with RandomUnderSampler ...           0.826648   \n",
       "BalancedRandomForestClassifier                               0.921138   \n",
       "BalancedRandomForestClassifier with optimal ratio            0.827981   \n",
       "BalancedBaggingClassifier with optimal ratio                 0.917623   \n",
       "\n",
       "                                                    Precision    Recall  \\\n",
       "DummyClassifier                                      0.000000  0.000000   \n",
       "LogisticRegression                                   0.885536  0.601196   \n",
       "RandomForestClassifier                               0.753393  0.790270   \n",
       "LogisticRegression with balanced class weight        0.062168  0.892228   \n",
       "RandomForestClassifier with balanced class weight    0.903390  0.680478   \n",
       "LogisticRegression with RandomUnderSampler           0.042693  0.898330   \n",
       "LogisticRegression with RandomUnderSampler with...   0.767201  0.772047   \n",
       "RandomForestClassifier with RandomUnderSampler ...   0.514151  0.851288   \n",
       "BalancedRandomForestClassifier                       0.053662  0.908411   \n",
       "BalancedRandomForestClassifier with optimal ratio    0.546700  0.851288   \n",
       "BalancedBaggingClassifier with optimal ratio         0.668683  0.837209   \n",
       "\n",
       "                                                    Average precision  \n",
       "DummyClassifier                                              0.001727  \n",
       "LogisticRegression                                           0.739402  \n",
       "RandomForestClassifier                                       0.732701  \n",
       "LogisticRegression with balanced class weight                0.740864  \n",
       "RandomForestClassifier with balanced class weight            0.743878  \n",
       "LogisticRegression with RandomUnderSampler                   0.543483  \n",
       "LogisticRegression with RandomUnderSampler with...           0.753711  \n",
       "RandomForestClassifier with RandomUnderSampler ...           0.729608  \n",
       "BalancedRandomForestClassifier                               0.761745  \n",
       "BalancedRandomForestClassifier with optimal ratio            0.762562  \n",
       "BalancedBaggingClassifier with optimal ratio                 0.774913  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "10 fits failed out of a total of 75.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 262, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 220, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 388, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/base.py\", line 79, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 535, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 393, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.75684059 0.7658642  0.77431039 0.77871909\n",
      " 0.77859509 0.77824882 0.77461575 0.77410373 0.77498096 0.74603436\n",
      " 0.7384545  0.73168729 0.6766176 ]\n",
      "  warnings.warn(\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/ensemble/_bagging.py\", line 321, in fit\n",
      "    return self._fit(X, y, self.max_samples, sample_weight=None)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/ensemble/_bagging.py\", line 394, in _fit\n",
      "    all_results = Parallel(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 209, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/ensemble/_bagging.py\", line 126, in _parallel_build_estimators\n",
      "    estimator.fit((X[indices])[:, features], y[indices])\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 262, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 220, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 388, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/base.py\", line 79, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 535, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 393, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.70983005 0.71870235 0.78072247 0.78835389 0.78408507\n",
      " 0.76092779 0.76700049 0.75671852 0.74760635]\n",
      "  warnings.warn(\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "10 fits failed out of a total of 75.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 262, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 220, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 388, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/base.py\", line 79, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 535, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 393, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.8153782  0.81583265 0.81407683 0.81392787\n",
      " 0.81600053 0.82038389 0.82335301 0.82643432 0.82514465 0.82234682\n",
      " 0.81243753 0.80287556 0.80278494]\n",
      "  warnings.warn(\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/ensemble/_bagging.py\", line 321, in fit\n",
      "    return self._fit(X, y, self.max_samples, sample_weight=None)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/ensemble/_bagging.py\", line 394, in _fit\n",
      "    all_results = Parallel(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 209, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/ensemble/_bagging.py\", line 126, in _parallel_build_estimators\n",
      "    estimator.fit((X[indices])[:, features], y[indices])\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 262, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 220, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 388, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/base.py\", line 79, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 535, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 393, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.75337635 0.77172004 0.80252024 0.80918639 0.81574875\n",
      " 0.79781713 0.79455659 0.81003518 0.81159291]\n",
      "  warnings.warn(\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "10 fits failed out of a total of 75.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 262, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 220, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 388, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/base.py\", line 79, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 535, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 393, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.73032248 0.73715469 0.7474259  0.75682156\n",
      " 0.75284278 0.75176844 0.74741616 0.74637275 0.75001787 0.72284509\n",
      " 0.70865633 0.70509655 0.6687043 ]\n",
      "  warnings.warn(\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/ensemble/_bagging.py\", line 321, in fit\n",
      "    return self._fit(X, y, self.max_samples, sample_weight=None)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/ensemble/_bagging.py\", line 394, in _fit\n",
      "    all_results = Parallel(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 209, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/ensemble/_bagging.py\", line 126, in _parallel_build_estimators\n",
      "    estimator.fit((X[indices])[:, features], y[indices])\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 262, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 220, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 388, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/base.py\", line 79, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 535, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 393, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.68013038 0.71598076 0.7330945  0.7491173  0.73602227\n",
      " 0.75305036 0.75073831 0.73714214 0.72799957]\n",
      "  warnings.warn(\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "10 fits failed out of a total of 75.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 262, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 220, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 388, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/base.py\", line 79, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 535, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 393, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.75669113 0.77304238 0.78014114 0.78854337\n",
      " 0.79223833 0.78942961 0.78695075 0.77811421 0.78379229 0.78148612\n",
      " 0.7455807  0.73002002 0.69200154]\n",
      "  warnings.warn(\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/ensemble/_bagging.py\", line 321, in fit\n",
      "    return self._fit(X, y, self.max_samples, sample_weight=None)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/ensemble/_bagging.py\", line 394, in _fit\n",
      "    all_results = Parallel(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 209, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/ensemble/_bagging.py\", line 126, in _parallel_build_estimators\n",
      "    estimator.fit((X[indices])[:, features], y[indices])\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 262, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 220, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 388, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/base.py\", line 79, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 535, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 393, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.72470109 0.67244606 0.74413254 0.76302894 0.75231257\n",
      " 0.73758208 0.75989345 0.73906361 0.71907714]\n",
      "  warnings.warn(\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1308: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "10 fits failed out of a total of 75.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 262, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 220, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 388, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/base.py\", line 79, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 535, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 393, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.74118106 0.75414546 0.75791574 0.76478307\n",
      " 0.7646022  0.76121966 0.76323599 0.76483234 0.76205087 0.75565033\n",
      " 0.72099967 0.69776859 0.68789537]\n",
      "  warnings.warn(\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "5 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/ensemble/_bagging.py\", line 321, in fit\n",
      "    return self._fit(X, y, self.max_samples, sample_weight=None)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/ensemble/_bagging.py\", line 394, in _fit\n",
      "    all_results = Parallel(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 1041, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 859, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 777, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/utils/fixes.py\", line 209, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/ensemble/_bagging.py\", line 126, in _parallel_build_estimators\n",
      "    estimator.fit((X[indices])[:, features], y[indices])\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 262, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 220, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/pipeline.py\", line 388, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/base.py\", line 79, in fit_resample\n",
      "    self.sampling_strategy_ = check_sampling_strategy(\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 535, in check_sampling_strategy\n",
      "    _sampling_strategy_float(sampling_strategy, y, sampling_type).items()\n",
      "  File \"/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/imblearn/utils/_validation.py\", line 393, in _sampling_strategy_float\n",
      "    raise ValueError(\n",
      "ValueError: The specified ratio required to generate new sample in the majority class while trying to remove samples. Please increase the ratio.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/glemaitre/mambaforge/envs/teaching/lib/python3.8/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan 0.74863437 0.74106989 0.78285671 0.80703077 0.79618089\n",
      " 0.79176222 0.78519775 0.80905424 0.76544149]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "index.append(\"BalancedBaggingClassifier with optimal ratio\")\n",
    "scores = update_scores(scores, cv_results)\n",
    "\n",
    "df_scores = pd.DataFrame(scores, index=index)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last approach is probably the most effective but request a huge amount of resource since it relies on powerful models.\n",
    "\n",
    "However keep in mind that whatever we did here was to optimize a given metric. Is the metric choosen the right one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost-sensitive metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with a business oriented dataset, it might be interested to ask ourselve if the metrics chosen to optimized our model previously were also meaningful for our application (business).\n",
    "\n",
    "Let's define a real cost-driven metric based on the confusion matrix. We will compute the confusion matrix given us the true positive and negative and the false positive en negative. We will then apply some business rules (completely arbitrary) to convert it into a monetary cost/benefit metric.\n",
    "\n",
    "In short, we could have the following rules:\n",
    "\n",
    "- not detecting a fraud will cost us the amount of the transaction\n",
    "- detecting a fraud will benefit us 20 euros\n",
    "- refusing a legitimate transaction will annoy our customer and cost us 20 euros\n",
    "- accepting a legitimate transaction will increase customer confidence and the benefit will depend of the transaction amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benefit_matrix(estimator, X, y):\n",
    "    y_pred = estimator.predict(X)\n",
    "    tp = (y == \"True\") & (y == y_pred)\n",
    "    tn = (y == \"False\") & (y == y_pred)\n",
    "    fp = (y_pred == \"True\") & (y != y_pred)\n",
    "    fn = (y_pred == \"False\") & (y != y_pred)\n",
    "    \n",
    "    # transform into benefit matrix\n",
    "    # little benefit when accepting a true transaction\n",
    "    # it will be related to the amount\n",
    "    tn_benefit = (X[\"Amount\"][tn] * 0.02).sum()\n",
    "    # detecting a fraud is not trivial and arbritary\n",
    "    tp_benefit = tp.sum() * 20\n",
    "    # blocking a legitimate transaction will annoy our\n",
    "    # customer\n",
    "    fp_benefit = fp.sum() * -20\n",
    "    # not blocking a fraud will cost us the transaction\n",
    "    # money\n",
    "    fn_benefit = -(X[\"Amount\"][fn]).sum()\n",
    "    return {\n",
    "        \"tp_benefit\": tp_benefit,\n",
    "        \"tn_benefit\": tn_benefit,\n",
    "        \"fp_benefit\": fp_benefit,\n",
    "        \"fn_benefit\": fn_benefit,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have our business metric, we can evalutate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    LogisticRegression()\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    model, X, y, scoring=benefit_matrix, n_jobs=-1, error_score=\"raise\"\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_tp_benefit</th>\n",
       "      <th>test_tn_benefit</th>\n",
       "      <th>test_fp_benefit</th>\n",
       "      <th>test_fn_benefit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1540</td>\n",
       "      <td>107860.0006</td>\n",
       "      <td>-660</td>\n",
       "      <td>-1995.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1260</td>\n",
       "      <td>107876.2120</td>\n",
       "      <td>-40</td>\n",
       "      <td>-4338.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>840</td>\n",
       "      <td>81607.5532</td>\n",
       "      <td>-80</td>\n",
       "      <td>-10062.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1340</td>\n",
       "      <td>115083.0694</td>\n",
       "      <td>-140</td>\n",
       "      <td>-7670.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>940</td>\n",
       "      <td>88660.8090</td>\n",
       "      <td>-60</td>\n",
       "      <td>-8089.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_tp_benefit  test_tn_benefit  test_fp_benefit  test_fn_benefit\n",
       "0             1540      107860.0006             -660         -1995.30\n",
       "1             1260      107876.2120              -40         -4338.05\n",
       "2              840       81607.5532              -80        -10062.68\n",
       "3             1340      115083.0694             -140         -7670.37\n",
       "4              940       88660.8090              -60         -8089.81"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_names = [name for name in cv_results.columns if \"test_\" in name]\n",
    "cv_results[metric_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97730.42656"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results[metric_names].sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try to evalute a model where we will resample the dataset. To select the sampling rate, we will maximize the total benefit instead of the average precision that we earlier used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_benefit(estimator, X, y):\n",
    "    return sum(benefit_matrix(estimator, X, y).values())\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"randomundersampler__sampling_strategy\": np.logspace(-2.1, -1, num=15)\n",
    "}\n",
    "classifier = GridSearchCV(\n",
    "    make_pipeline_with_sampler(\n",
    "        StandardScaler(),\n",
    "        RandomUnderSampler(random_state=42),\n",
    "        LogisticRegression(max_iter=1000),\n",
    "    ),\n",
    "    param_grid=param_grid,\n",
    "    scoring=total_benefit,\n",
    "    n_jobs=-1\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=benefit_matrix, return_estimator=True,\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97853.06096"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results[metric_names].sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <p><b>EXERCISE</b>:</p>\n",
    "    Repeat the above experiment. However, optimize the balanced accuracy within the grid-search. Is it better or worse than optimising in terms of final business metric?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"randomundersampler__sampling_strategy\": np.logspace(-2.5, 0, num=15)\n",
    "}\n",
    "classifier = GridSearchCV(\n",
    "    make_pipeline_with_sampler(\n",
    "        StandardScaler(),\n",
    "        RandomUnderSampler(random_state=42),\n",
    "        LogisticRegression(max_iter=1000),\n",
    "    ),\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    classifier, X, y, scoring=benefit_matrix, return_estimator=True,\n",
    ")\n",
    "cv_results = pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46247.5634"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results[metric_names].sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'randomundersampler__sampling_strategy': 0.6628703161826448}\n",
      "{'randomundersampler__sampling_strategy': 0.43939705607607904}\n",
      "{'randomundersampler__sampling_strategy': 0.43939705607607904}\n",
      "{'randomundersampler__sampling_strategy': 0.6628703161826448}\n",
      "{'randomundersampler__sampling_strategy': 0.19306977288832505}\n"
     ]
    }
   ],
   "source": [
    "for estimator in cv_results[\"estimator\"]:\n",
    "    print(estimator.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
